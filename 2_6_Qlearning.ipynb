{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhyoo78/jhyoo78/blob/main/2_6_Qlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqTQXYBTvqap"
      },
      "outputs": [],
      "source": [
        "# 구현에 사용할 패키지 임포트\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6mP9FNDvqas"
      },
      "outputs": [],
      "source": [
        "# 초기 상태의 미로 모습\n",
        "\n",
        "# 전체 그림의 크기 및 그림을 나타내는 변수 선언\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = plt.gca()\n",
        "\n",
        "# 붉은 벽 그리기\n",
        "plt.plot([1, 1], [0, 1], color='red', linewidth=2)\n",
        "plt.plot([1, 2], [2, 2], color='red', linewidth=2)\n",
        "plt.plot([2, 2], [2, 1], color='red', linewidth=2)\n",
        "plt.plot([2, 3], [1, 1], color='red', linewidth=2)\n",
        "\n",
        "# 상태를 의미하는 문자열(S0~S8) 표시\n",
        "plt.text(0.5, 2.5, 'S0', size=14, ha='center')\n",
        "plt.text(1.5, 2.5, 'S1', size=14, ha='center')\n",
        "plt.text(2.5, 2.5, 'S2', size=14, ha='center')\n",
        "plt.text(0.5, 1.5, 'S3', size=14, ha='center')\n",
        "plt.text(1.5, 1.5, 'S4', size=14, ha='center')\n",
        "plt.text(2.5, 1.5, 'S5', size=14, ha='center')\n",
        "plt.text(0.5, 0.5, 'S6', size=14, ha='center')\n",
        "plt.text(1.5, 0.5, 'S7', size=14, ha='center')\n",
        "plt.text(2.5, 0.5, 'S8', size=14, ha='center')\n",
        "plt.text(0.5, 2.3, 'START', ha='center')\n",
        "plt.text(2.5, 0.3, 'GOAL', ha='center')\n",
        "\n",
        "# 그림을 그릴 범위 및 눈금 제거 설정\n",
        "ax.set_xlim(0, 3)\n",
        "ax.set_ylim(0, 3)\n",
        "plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                labelbottom=False, right=False, left=False, labelleft=False)\n",
        "\n",
        "# S0에 녹색 원으로 현재 위치를 표시\n",
        "line, = ax.plot([0.5], [2.5], marker=\"o\", color='g', markersize=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KXCUd8Evqat"
      },
      "outputs": [],
      "source": [
        "# 정책을 결정하는 파라미터의 초깃값 theta_0를 설정\n",
        "\n",
        "# 줄은 상태 0~7, 열은 행동방향(상,우,하,좌 순)를 나타낸다\n",
        "theta_0 = np.array([[np.nan, 1, 1, np.nan],  # s0\n",
        "                    [np.nan, 1, np.nan, 1],  # s1\n",
        "                    [np.nan, np.nan, 1, 1],  # s2\n",
        "                    [1, 1, 1, np.nan],  # s3\n",
        "                    [np.nan, np.nan, 1, 1],  # s4\n",
        "                    [1, np.nan, np.nan, np.nan],  # s5\n",
        "                    [1, np.nan, np.nan, np.nan],  # s6\n",
        "                    [1, 1, np.nan, np.nan],  # s7、※s8은 목표지점이므로 정책이 없다\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NYpEeTzvqau"
      },
      "outputs": [],
      "source": [
        "# 정책 파라미터 theta_0을 무작위 행동 정책 pi로 변환하는 함수\n",
        "\n",
        "\n",
        "def simple_convert_into_pi_from_theta(theta):\n",
        "    '''단순 비율 계산'''\n",
        "\n",
        "    [m, n] = theta.shape  # theta의 행렬 크기를 구함\n",
        "    pi = np.zeros((m, n))\n",
        "    for i in range(0, m):\n",
        "        pi[i, :] = theta[i, :] / np.nansum(theta[i, :])  # 비율 계산\n",
        "\n",
        "    pi = np.nan_to_num(pi)  # nan을 0으로 변환\n",
        "\n",
        "    return pi\n",
        "\n",
        "# 무작위 행동정책 pi_0을 계산\n",
        "pi_0 = simple_convert_into_pi_from_theta(theta_0)\n",
        "print(pi_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnGsQUKWvqaw"
      },
      "outputs": [],
      "source": [
        "# 행동가치 함수 Q의 초기 상태: 초기에는 Q 값을 알수 없으므로  random으로 생성\n",
        "\n",
        "[a, b] = theta_0.shape  # # 열과 행의 갯수를 변수 a, b에 저장\n",
        "Q = np.random.rand(a, b) * theta_0 * 0.1    # Q 값의 초기 값이 너무 크면 애니메이션으로 나타내기 어려우므로 0.1을 곱해 줌(초기 값임).\n",
        "print(Q)\n",
        "# * theta0 로 요소 단위 곱셈을 수행, Q에서 벽 방향으로 이동하는 행동에는 nan을 부여"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXE7hSy2vqax"
      },
      "outputs": [],
      "source": [
        "# ε-greedy 알고리즘 구현\n",
        "\n",
        "\n",
        "def get_action(s, Q, epsilon, pi_0):\n",
        "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "\n",
        "    # 행동을 결정\n",
        "    if np.random.rand() < epsilon:\n",
        "        # 확률 ε로 무작위 행동을 선택함\n",
        "        next_direction = np.random.choice(direction, p=pi_0[s, :])      # p 확률로 direction[]에서 선택한다.\n",
        "    else:\n",
        "        # Q값이 최대가 되는 행동을 선택함\n",
        "        next_direction = direction[np.nanargmax(Q[s, :])]               # nan을 무시하고 최대값을 index를 return\n",
        "\n",
        "    # 행동을 인덱스로 변환\n",
        "    if next_direction == \"up\":\n",
        "        action = 0\n",
        "    elif next_direction == \"right\":\n",
        "        action = 1\n",
        "    elif next_direction == \"down\":\n",
        "        action = 2\n",
        "    elif next_direction == \"left\":\n",
        "        action = 3\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "def get_s_next(s, a, Q, epsilon, pi_0):\n",
        "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "    next_direction = direction[a]  # 행동 a의 방향\n",
        "\n",
        "    # 행동으로 다음 상태를 결정\n",
        "    if next_direction == \"up\":\n",
        "        s_next = s - 3  # 위로 이동하면 상태값이 3 줄어든다\n",
        "    elif next_direction == \"right\":\n",
        "        s_next = s + 1  # 오른쪽으로 이동하면 상태값이 1 늘어난다\n",
        "    elif next_direction == \"down\":\n",
        "        s_next = s + 3  # 아래로 이동하면 상태값이 3 늘어난다\n",
        "    elif next_direction == \"left\":\n",
        "        s_next = s - 1  # 왼쪽으로 이동하면 상태값이 1 줄어든다\n",
        "\n",
        "    return s_next"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqVFN0yrvqax"
      },
      "outputs": [],
      "source": [
        "# Q러닝 알고리즘으로 행동가치 함수 Q를 수정\n",
        "\n",
        "\n",
        "def Q_learning(s, a, r, s_next, Q, eta, gamma):\n",
        "\n",
        "    if s_next == 8:  # 목표 지점에 도달한 경우, 다음 상태(st+1)이 존재하지 않으므로 Q함수 식에서 gamma*max Q(st+1,a)를 삭제 가능\n",
        "        Q[s, a] = Q[s, a] + eta * (r - Q[s, a])\n",
        "\n",
        "    else:\n",
        "        Q[s, a] = Q[s, a] + eta * (r + gamma * np.nanmax(Q[s_next,: ]) - Q[s, a])\n",
        "\n",
        "    return Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEKfa3g4vqay"
      },
      "outputs": [],
      "source": [
        "# Q러닝 알고리즘으로 미로를 빠져나오는 함수, 상태 및 행동 그리고 Q값의 히스토리를 출력한다\n",
        "\n",
        "\n",
        "def goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi):\n",
        "    s = 0  # 시작 지점\n",
        "    a = a_next = get_action(s, Q, epsilon, pi)  # 첫 번째 행동\n",
        "    s_a_history = [[0, np.nan]]  # 에이전트의 행동 및 상태의 히스토리를 기록하는 리스트\n",
        "\n",
        "    while (1):  # 목표 지점에 이를 때까지 반복\n",
        "        a = a_next  # 행동 결정\n",
        "\n",
        "        s_a_history[-1][1] = a\n",
        "        # 현재 상태(마지막으로 인덱스가 -1)을 히스토리에 추가\n",
        "\n",
        "        s_next = get_s_next(s, a, Q, epsilon, pi)\n",
        "        # 다음 단계의 상태를 구함\n",
        "        '''Print 추가'''\n",
        "        print(s_next)\n",
        "\n",
        "        s_a_history.append([s_next, np.nan])\n",
        "        # 다음 상태를 히스토리에 추가, 행동은 아직 알 수 없으므로 nan으로 둔다\n",
        "\n",
        "        # 보상을 부여하고 다음 행동을 계산함\n",
        "        if s_next == 8:\n",
        "            r = 1  # 목표 지점에 도달했다면 보상을 부여\n",
        "            a_next = np.nan\n",
        "        else:\n",
        "            r = 0\n",
        "            a_next = get_action(s_next, Q, epsilon, pi)\n",
        "            # 다음 행동 a_next를 계산\n",
        "\n",
        "        # 가치함수를 수정\n",
        "        Q = Q_learning(s, a, r, s_next, Q, eta, gamma)\n",
        "\n",
        "        # 종료 여부 판정\n",
        "        if s_next == 8:  # 목표 지점에 도달하면 종료\n",
        "            break\n",
        "        else:\n",
        "            s = s_next\n",
        "\n",
        "    return [s_a_history, Q]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCFtNttrvqay"
      },
      "outputs": [],
      "source": [
        "# Q러닝 알고리즘으로 미로 빠져나오기\n",
        "\n",
        "eta = 0.1  # 학습률\n",
        "gamma = 0.9  # 시간할인율\n",
        "epsilon = 0.5  # ε-greedy 알고리즘 epsilon 초깃값\n",
        "v = np.nanmax(Q, axis=1)  # 각 상태마다 가치의 최댓값을 계산\n",
        "is_continue = True\n",
        "episode = 1\n",
        "\n",
        "V = []  # 에피소드 별로 상태가치를 저장\n",
        "V.append(np.nanmax(Q, axis=1))  # 상태 별로 행동가치의 최댓값을 계산\n",
        "\n",
        "while is_continue:  # is_continue의 값이 False가 될 때까지 반복\n",
        "    print(\"에피소드: \" + str(episode))\n",
        "\n",
        "    # ε 값을 조금씩 감소시킴\n",
        "    epsilon = epsilon / 2\n",
        "\n",
        "    # Q러닝으로 미로를 빠져나온 후, 결과로 나온 행동(이동한 이력) 히스토리와 갱신한 Q값을 변수에 저장\n",
        "    [s_a_history, Q] = goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi_0)\n",
        "\n",
        "    # 상태가치의 변화\n",
        "    new_v = np.nanmax(Q, axis=1)      # 각 상태마다 행동가치의 최댓값을 계산\n",
        "    print(np.sum(np.abs(new_v - v)))  # 상태가치 함수의 변화를 출력\n",
        "    v = new_v\n",
        "    V.append(v)                       # 현재 에피소드가 끝난 시점의 상태가치 함수를 추가\n",
        "\n",
        "    print(\"목표 지점에 이르기까지 걸린 단계 수는 \" + str(len(s_a_history) - 1) + \"단계입니다\")\n",
        "\n",
        "    # 100 에피소드 반복\n",
        "    episode = episode + 1\n",
        "    if episode > 100:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDE72pYQvqaz"
      },
      "outputs": [],
      "source": [
        "# 참고 URL http://louistiao.me/posts/notebooks/embedding-matplotlib-animations-in-jupyter-notebooks/\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "import matplotlib.cm as cm  # color map\n",
        "\n",
        "\n",
        "def init():\n",
        "    # 배경색 초기화\n",
        "    line.set_data([], [])\n",
        "    return (line,)\n",
        "\n",
        "\n",
        "def animate(i):\n",
        "    # 프레임 단위로 그림을 그림\n",
        "    # 각 칸에 상태가치 값으로 결정된 색을 칠함\n",
        "    line, = ax.plot([0.5], [2.5], marker=\"s\",\n",
        "                    color=cm.jet(V[i][0]), markersize=85)  # S0\n",
        "    line, = ax.plot([1.5], [2.5], marker=\"s\",\n",
        "                    color=cm.jet(V[i][1]), markersize=85)  # S1\n",
        "    line, = ax.plot([2.5], [2.5], marker=\"s\",\n",
        "                    color=cm.jet(V[i][2]), markersize=85)  # S2\n",
        "    line, = ax.plot([0.5], [1.5], marker=\"s\",\n",
        "                    color=cm.jet(V[i][3]), markersize=85)  # S3\n",
        "    line, = ax.plot([1.5], [1.5], marker=\"s\",\n",
        "                    color=cm.jet(V[i][4]), markersize=85)  # S4\n",
        "    line, = ax.plot([2.5], [1.5], marker=\"s\",\n",
        "                    color=cm.jet(V[i][5]), markersize=85)  # S5\n",
        "    line, = ax.plot([0.5], [0.5], marker=\"s\",\n",
        "                    color=cm.jet(V[i][6]), markersize=85)  # S6\n",
        "    line, = ax.plot([1.5], [0.5], marker=\"s\",\n",
        "                    color=cm.jet(V[i][7]), markersize=85)  # S7\n",
        "    line, = ax.plot([2.5], [0.5], marker=\"s\",\n",
        "                    color=cm.jet(1.0), markersize=85)  # S8\n",
        "    return (line,)\n",
        "\n",
        "\n",
        "# 초기화 함수와 프레임 단위로 그림을 그리는 함수로 애니메이션을 생성\n",
        "anim = animation.FuncAnimation(\n",
        "    fig, animate, init_func=init, frames=len(V), interval=200, repeat=False)\n",
        "\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yinx8B3hvqa0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}