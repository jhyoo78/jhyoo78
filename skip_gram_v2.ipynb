{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMqGnz3bhLeEGge776cS0sT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhyoo78/jhyoo78/blob/main/skip_gram_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xzw6N7fAenyX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(10)\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import decomposition\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (10,8)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk  # natural Language toolkit(python)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')  # 추가 --> 없으면 3번째 노트북 페이지의 nltk.word_tokenize(i)에서 오류 발생\n",
        "from nltk.corpus import stopwords  #Import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    'drink milk',\n",
        "    'drink cold water',\n",
        "    'drink cold cola',\n",
        "    'drink juice',\n",
        "    'drink cola',\n",
        "    'eat bacon',\n",
        "    'eat mango',\n",
        "    'eat cherry',\n",
        "    'eat apple',\n",
        "    'juice with sugar',\n",
        "    'cola with sugar',\n",
        "    'mango is fruit',\n",
        "    'apple is fruit',\n",
        "    'cherry is fruit',\n",
        "    'Berlin is Germany',\n",
        "    'Boston is USA',\n",
        "    'Mercedes from Germany',\n",
        "    'Mercedes is a car',\n",
        "    'Ford from USA',\n",
        "    'Ford is a car'\n",
        "]"
      ],
      "metadata": {
        "id": "sTKoQCLner9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary(corpus):\n",
        "    '''말 뭉치 내의 모든 단어(ID로 식별)에 대해 등장 순서대로 하나의 dictionary를 생성한다. 빈도순서가 아님'''\n",
        "    vocabulary = {}\n",
        "    i = 0\n",
        "    for s in corpus:\n",
        "        for w in s.split():   # s.split()은 문자열 s를 (구분자)에 따라 분리하여 list로 만듬\n",
        "            if w not in vocabulary:\n",
        "                vocabulary[w] = i\n",
        "                #print(vocabulary)  # 추가된 dict 내용을 포함하여 전체를 출력\n",
        "                i+=1\n",
        "    return vocabulary\n",
        "\n",
        "def prepare_set(corpus, n_gram = 1):\n",
        "    '''이웃 단어들에 대해 입력 열과 출력 열로 구성된 dataset을 생성한다. Creates a dataset with Input column and Outputs columns for neighboring words.\n",
        "       이 경우 이웃 관계의 수 = n_gram x 2 이다 '''\n",
        "    columns = ['Input'] + [f'Output{i+1}' for i in range(n_gram*2)]\n",
        "    result = pd.DataFrame(columns = columns)\n",
        "    for sentence in corpus:\n",
        "        for i,w in enumerate(sentence.split()):\n",
        "            inp = [w]\n",
        "            out = []\n",
        "            for n in range(1,n_gram+1):\n",
        "                # look back\n",
        "                if (i-n)>=0:\n",
        "                    out.append(sentence.split()[i-n])\n",
        "                else:\n",
        "                    out.append('<padding>')\n",
        "\n",
        "                # look forward\n",
        "                if (i+n)<len(sentence.split()):\n",
        "                    out.append(sentence.split()[i+n])\n",
        "                else:\n",
        "                    out.append('<padding>')\n",
        "            row = pd.DataFrame([inp+out], columns = columns)\n",
        "            result = result.append(row, ignore_index = True)\n",
        "    return result\n",
        "\n",
        "def prepare_set_ravel(corpus, n_gram = 1):\n",
        "    '''이웃 단어들에 대해 입력 열과 출력 열로 구성된 dataset을 생성한다.\n",
        "          이 경우 이웃 관계의 수 = n_gram x 2 이다 '''\n",
        "    columns = ['Input', 'Output']           # column 이름을 지정하여 df를 생성한다.\n",
        "    result = pd.DataFrame(columns = columns)\n",
        "    for sentence in corpus:\n",
        "        for i,w in enumerate(sentence.split()):\n",
        "            inp = w\n",
        "            for n in range(1,n_gram+1):\n",
        "                # look back\n",
        "                if (i-n)>=0:\n",
        "                    out = sentence.split()[i-n]\n",
        "                    row = pd.DataFrame([[inp,out]], columns = columns)\n",
        "                    result = result.append(row, ignore_index = True)\n",
        "\n",
        "                # look forward\n",
        "                if (i+n)<len(sentence.split()):\n",
        "                    out = sentence.split()[i+n]\n",
        "                    row = pd.DataFrame([[inp,out]], columns = columns)\n",
        "                    result = result.append(row, ignore_index = True)\n",
        "    return result"
      ],
      "metadata": {
        "id": "u2LfGpSDesCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불용어(the, a, is, from, with 등)를 제거한다.\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(corpus):\n",
        "    result = []\n",
        "    for i in corpus:\n",
        "        out = nltk.word_tokenize(i)\n",
        "        out = [x.lower() for x in out]\n",
        "        out = [x for x in out if x not in stop_words]\n",
        "        result.append(\" \". join(out))\n",
        "    return result\n",
        "\n",
        "corpus = preprocess(corpus)\n",
        "corpus"
      ],
      "metadata": {
        "id": "SQqzr5bweylT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary를 만든다. 빈도 순서가 아니고 등장 순서로 index가 분여됨.\n",
        "\n",
        "vocabulary = create_vocabulary(corpus)\n",
        "vocabulary"
      ],
      "metadata": {
        "id": "FjRyi9aReyrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 문장에 대해 단어의 조합을 만든다.\n",
        "\n",
        "train_emb = prepare_set(corpus, n_gram = 2)\n",
        "train_emb.head(10)"
      ],
      "metadata": {
        "id": "OfFuZRaUeyvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n-gram을 2로 하여 답어 조합을 만든다.\n",
        "\n",
        "train_emb = prepare_set_ravel(corpus, n_gram = 2)\n",
        "train_emb.head()"
      ],
      "metadata": {
        "id": "ah2yRxbGeyzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 앞에서 출력한 각 단어를 index로 바꾼다.\n",
        "\n",
        "train_emb.Input = train_emb.Input.map(vocabulary)   # df에서 Input 열을 vocabulary의 index로 변환한다. 단, train_emb와 vocabulary에 동일한 열이 존재해야 한다.\n",
        "print(train_emb.head() )\n",
        "train_emb.Output = train_emb.Output.map(vocabulary) # df에서 Output 열을 vocabulary의 index로 변환한다.\n",
        "train_emb.head()"
      ],
      "metadata": {
        "id": "7JtL-JjBey3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocabulary)\n",
        "\n",
        "def get_input_tensor(tensor):  # 단어 인덱스의 1차원 텐서를 one-hot 인코딩된 2D 텐서로 변환한다. 이 함수는 입력계층에서만 사용한다.\n",
        "    size = [*tensor.shape][0]\n",
        "    inp = torch.zeros(size, vocab_size).scatter_(1, tensor.unsqueeze(1), 1.)\n",
        "    return Variable(inp).float()"
      ],
      "metadata": {
        "id": "dqGEfRiUey7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dims = 5\n",
        "device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "mkRdQXFQfFlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단순 확인 용이므로 삭제할 것\n",
        "\n",
        "a= torch.rand(3, 3)   # 0.0 ~ 1.0 사이의 값을 균등분포로 출력,\n",
        "print(a)\n",
        "print(torch.randn(3,3))  # 평균이 0 이고 분산이 1인 가우시안(표준) 정규 분포로 출력"
      ],
      "metadata": {
        "id": "YFLdsJFAde-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initrange = 0.5 / embedding_dims\n",
        "\n",
        "'''무작위 값을 갖는 Tensor를 생성하고, Variable로 감싼다. requires_grade=True로 설정하여 역전파 중에 이 Variable들에 대한 기울기를 계산할 수 있게 한다'''\n",
        "'''Variable()은 autograd(자동 미분)을 위해 사용되었으나 2018년 이후 torch.tensor로 통합되었음'''\n",
        "\n",
        "W1 = Variable(torch.randn(vocab_size, embedding_dims, device=device).uniform_(-initrange, initrange).float(), requires_grad=True)  # shape V*H\n",
        "# torch.rand는 uniform 분포로 균등하게 출력\n",
        "# torch.randn은 Gausian normal distribution으로 출력, unifrom_(a, b)는 [a, b] 범위의 값으로 unifrom random matrix를 생성, device는 앞에서 정의한 cpu를 사용,\n",
        "W2 = Variable(torch.randn(embedding_dims, vocab_size, device=device).uniform_(-initrange, initrange).float(), requires_grad=True)  # shape H*V\n",
        "print(f'W1 shape is: {W1.shape}, W2 shape is: {W2.shape}')\n",
        "print(W1)\n",
        "print(W2)"
      ],
      "metadata": {
        "id": "XVMO4V6mfFua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2000\n",
        "learning_rate = 2e-1   # 0.2\n",
        "lr_decay = 0.99\n",
        "loss_hist = []"
      ],
      "metadata": {
        "id": "n-SSFIvEfF2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for epo in range(num_epochs):\n",
        "    for x,y in zip(DataLoader(train_emb.Input.values, batch_size=train_emb.shape[0]), DataLoader(train_emb.Output.values, batch_size=train_emb.shape[0])):\n",
        "\n",
        "        # one-hot encode input tensor\n",
        "        input_tensor = get_input_tensor(x) #shape N*V\n",
        "\n",
        "        # simple NN architecture\n",
        "        h = input_tensor.mm(W1) # shape 1*H\n",
        "        y_pred = h.mm(W2) # shape 1*V\n",
        "\n",
        "        # define loss func\n",
        "        loss_f = torch.nn.CrossEntropyLoss()     # see details: https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "        #compute loss\n",
        "        loss = loss_f(y_pred, y)\n",
        "\n",
        "        # bakpropagation step\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights using gradient descent. For this step we just want to mutate\n",
        "        # the values of w1 and w2 in-place; we don't want to build up a computational\n",
        "        # graph for the update steps, so we use the torch.no_grad() context manager\n",
        "        # to prevent PyTorch from building a computational graph for the updates\n",
        "        with torch.no_grad():\n",
        "            # SGD optimization is implemented in PyTorch, but it's very easy to implement manually providing better understanding of process\n",
        "            W1 -= learning_rate*W1.grad.data  # 참고 --> https://9bow.github.io/PyTorch-tutorials-kr-0.3.1/beginner/pytorch_with_examples.html#pytorch-variables-autograd\n",
        "            W2 -= learning_rate*W2.grad.data\n",
        "            # zero gradients for next step\n",
        "            W1.grad.data.zero_()\n",
        "            W1.grad.data.zero_()\n",
        "    if epo%10 == 0:\n",
        "        learning_rate *= lr_decay\n",
        "    loss_hist.append(loss)\n",
        "    if epo%50 == 0:\n",
        "        print(f'Epoch {epo}, loss = {loss}')\n",
        ""
      ],
      "metadata": {
        "id": "wpj8AvudezBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = W1.detach().numpy()\n",
        "W2 = W2.T.detach().numpy()\n",
        "print(W1)"
      ],
      "metadata": {
        "id": "LvRrg20WezFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd = decomposition.TruncatedSVD(n_components=2)\n",
        "W1_dec = svd.fit_transform(W1)\n",
        "x = W1_dec[:,0]\n",
        "y = W1_dec[:,1]\n",
        "plot = sns.scatterplot(x, y)\n",
        "\n",
        "for i in range(0,W1_dec.shape[0]):\n",
        "     plot.text(x[i], y[i]+2e-2, list(vocabulary.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');"
      ],
      "metadata": {
        "id": "rC9cyC6BesGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W2_dec = svd.fit_transform(W2)\n",
        "x1 = W2_dec[:,0]\n",
        "y1 = W2_dec[:,1]\n",
        "plot1 = sns.scatterplot(x1, y1)\n",
        "for i in range(0,W2_dec.shape[0]):\n",
        "     plot1.text(x1[i], y1[i]+1, list(vocabulary.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');\n"
      ],
      "metadata": {
        "id": "AdiLyGZfesJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "STqfADfechfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a= torch.rand(3, 3)   # 0.0 ~ 1.0 사이의 값을 균등분포로 출력,\n",
        "print(a)\n",
        "print(torch.randn(3,3))  평균이 0 이고 분산이 1인 가우시안 정규 분포로 출력"
      ],
      "metadata": {
        "id": "ZVQUY4cuchoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Olv8RZNlchvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VKwV-z1Ychyz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}