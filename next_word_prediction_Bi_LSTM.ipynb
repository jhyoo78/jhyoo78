{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMKLmqXwYmv+zPQYkZuahOa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhyoo78/jhyoo78/blob/main/next_word_prediction_Bi_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 인용 --> https://www.kaggle.com/code/ysthehurricane/next-word-prediction-bi-lstm-tutorial-easy-way\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# 하나의 문장 내에서 다음 단어들을 맞출 때 import할 것들\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 단어가 아닌 sentence 단위로 전처리할 경우 추가할 것\n",
        "import nltk\n",
        "nltk.download('punkt')   # ‘punkt= 문장 tokenizer’를 다운로드\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re                # 파이썬 정규표현을 위한 표준 라이브러리임"
      ],
      "metadata": {
        "id": "HWl0o6IpQEwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxf9soFpP_Pv"
      },
      "outputs": [],
      "source": [
        "from google.colab import files     # Colab에서 local PC내의 log file 읽어 들이는 라이브러리\n",
        "uploaded = files.upload()          # 파일을 여러 개 선택 가능함, 실험을 위해 학습 용 1개와 test 용 1개 로그 파일을 선택"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하나의 문장 내에서 다음 단어를 맞출때 사용...\n",
        "input_sequence = []\n",
        "\n",
        "f = open('test1.txt', 'r')          # 1,100 줄의 log 파일\n",
        "lines = f.readlines()\n",
        "\n",
        "tokenizer = Tokenizer(oov_token='<oov>')     #  토큰화할 때 적용할 파라미터 --> 사전에 없는 단어는 '<oov>' 로 출력\n",
        "tokenizer.fit_on_texts(lines)                #  전체 단어의 index와 단어 벡터를 생성(Keras), #  단어들을 dictionary 로 변환. 예 --> {'<oov>': 1, 'systemd': 2, 'starting': 3, 'for': 4,......}\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(\"total words=\", total_words)\n",
        "print(\"word_index=\", tokenizer.word_index)\n",
        "\n",
        "for line in lines:\n",
        "    #print(line)                                    # log msg를 한 줄씩 출력\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]  # 각 문장(line)의 단어에 index를 부여해줌.\n",
        "    print(\"token_list=\", token_list)                      #  token_list = [3, 486, 299, 183], [3, 50, 300, 301].... 한 line에 하나의 list가 출력됨\n",
        "\n",
        "    for i in range(1, len(token_list)):             #  한 문장 내의 단어 수만큼 반복\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        #print(\"n_gram_seq=\", n_gram_sequence)\n",
        "        input_sequence.append(n_gram_sequence)     # --> [[3, 486], [3, 486, 299], [3, 486, 299, 183], [3, 50], [3, 50, 300], [3, 50, 300, 301],....]\n",
        "\n",
        "print(\"input_sequence=\", input_sequence)\n",
        "print(\"Total input sequences: \", len(input_sequence))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "0ZQgqwNJQkqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad sequences, 문장의 길이가 같아지도록 패딩 처리\n",
        "max_sequence_len = max([len(x) for x in input_sequence])\n",
        "input_sequence = np.array(pad_sequences(input_sequence, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "print(input_sequence)\n",
        "print(input_sequence[1])\n",
        "print(\"max_log_seq_length=\", max_sequence_len)"
      ],
      "metadata": {
        "id": "foI9b3pCQFV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create features and label\n",
        "xs, labels = input_sequence[:,:-1],input_sequence[:,-1]\n",
        "# [:, :] 는 [all rows, all columns]에 해당하며,  [:, :-1]은 all rows 와 마지막 column을 제외한 all columns에 해당함.\n",
        "# [:, -1] 은 last column의 모든 rows에 해당함.\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
        "# labels 내의 숫자를 각 행에 One-hot encoding해 준다. 즉 labels는 1차원 정수열이어야 한다.\n",
        "\n",
        "print(\"xs and shape=\", xs.shape)\n",
        "print(\"xs=\", xs[:10])    # len(input_seq) x padding\n",
        "print(\"labels=\", labels)\n",
        "print(\"ys=\", ys)\n",
        "print(\"labels_shape=\",labels.shape)\n",
        "print(\"ys_shape=\", ys.shape)"
      ],
      "metadata": {
        "id": "jd9S-sdGQFYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bi- LSTM Neural Network Model training\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 200, input_length=max_sequence_len-1))  # vector 100 --> 200으로 변경\n",
        "model.add(Bidirectional(LSTM(150)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "adam = Adam(learning_rate =0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "history = model.fit(xs, ys, epochs=10, verbose=1)  # epoch=50,   xs와 ys에 대해 학습을 시작함.\n",
        "#print model.summary()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "Ti-LNGDKQFbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')\n",
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "id": "TH6V6bMCQFh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting next word  "
      ],
      "metadata": {
        "id": "t3eXSxLSwmQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"finished apply\"\n",
        "next_words = 7    # 예측할 다음 단어 수...\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]      # 시험용 seed_text를 token화\n",
        "    print(token_list)\n",
        "\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')  # padding : 예 --> [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 9]]\n",
        "    print(\"token_list=\", token_list)\n",
        "\n",
        "    y_prob = model.predict(token_list, verbose=0)    # model.predicted_classes 는 deprecated 되었으니 이 코드를 사용해야 함.\n",
        "    print(y_prob, len(y_prob[0]))                    # total_words(1575개) 각각에 대한 확률을 출력함.  예 --> [[1.3384e-06 1.2275e-06 5.1810e-04 ... 3.392e-05]]\n",
        "\n",
        "    predicted = y_prob.argmax(axis=-1)               # 가장 큰 확률 값을 갖는 단어의 index를 선택\n",
        "                                                     # axis가 0 이면 2차원 행렬에서 열(가장 높은 차원), axis=1이면 행(다음으로 높은 차원), axis=-1이면 마지막 차원(가장 낮은 차원, 2차원에서는 행)을 기준 축으로 함\n",
        "                                                     # argmax()는 가장 큰 값이 아닌 그 값의 인덱스 값을 반환함.\n",
        "    print(\"predicted=\", predicted, max(y_prob[0]))   # 예측된 단어의 index와  확률을 출력\n",
        "\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():   # 실제 단어로 변환하여 출력에 append 함.\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "metadata": {
        "id": "8cEUG9XQQFo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e-2oixrfRI9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 끝..."
      ],
      "metadata": {
        "id": "l_4bu_bMTaOA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SOsyRn1TTmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fooQGgLDTTw8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}