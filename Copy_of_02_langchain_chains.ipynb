{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhyoo78/jhyoo78/blob/main/Copy_of_02_langchain_chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uZR3iGJJtdDE",
      "metadata": {
        "id": "uZR3iGJJtdDE"
      },
      "outputs": [],
      "source": [
        "!pip install cohere openai tiktoken\n",
        "!pip install -qU langchain openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a4ba72d",
      "metadata": {
        "id": "7a4ba72d"
      },
      "source": [
        "\n",
        "# [LangChain Handbook](https://github.com/pinecone-io/examples/tree/master/generation/langchain/handbook)\n",
        "\n",
        "## Getting Started with Chains\n",
        "\n",
        "Chains are the core of LangChain. They are simply a chain of components, executed in a particular order.\n",
        "\n",
        "The simplest of these chains is the `LLMChain`. It works by taking a user's input, passing in to the first element in the chain â€” a `PromptTemplate` â€” to format the input into a particular prompt. The formatted prompt is then passed to the next (and final) element in the chain â€” a LLM.\n",
        "\n",
        "We'll start by importing all the libraries that we'll be using in this example.\n",
        "\n",
        " Chains ëŠ” LangChainì˜ í•µì‹¬ ê¸°ëŠ¥ì´ë‹¤. ì´ê²ƒì€ ë‹¨ìˆœíˆ êµ¬ì„± ìš”ì†Œë“¤ì„ ì—°ê²°í•˜ëŠ” ê²ƒì´ê³ , íŠ¹ì •ëœ ìˆœì„œë¡œ ì‹¤í–‰ëœë‹¤.\n",
        "\n",
        "ì´ë“¤ chain ì¤‘ ê°€ì¥ ê°„ë‹¨í•œ ê²ƒì´ LLMChainì´ë‹¤.\n",
        "ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì•„ chainì˜ ì²« ë²ˆì§¸ ìš”ì†Œì¸ PromptTemplateë¡œ ì „ë‹¬í•˜ì—¬ ì…ë ¥ì„ íŠ¹ì • Promptë¡œ  formatí•˜ì—¬ ë™ì‘í•œë‹¤. ê·¸ ë‹¤ìŒ formatëœ PromptëŠ” chainì˜  ë§ˆì§€ë§‰ ìš”ì†Œì¸ LLMìœ¼ë¡œ ì „ë‹¬ëœë‹¤.\n",
        "\n",
        "ì´ ì˜ˆì—ì„œ ì‚¬ìš©í•  ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "66fb9c2a",
      "metadata": {
        "id": "66fb9c2a"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import re\n",
        "\n",
        "from getpass import getpass\n",
        "from langchain import OpenAI, PromptTemplate\n",
        "from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
        "from langchain.callbacks import get_openai_callback"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wPdWz1IdxyBR",
      "metadata": {
        "id": "wPdWz1IdxyBR"
      },
      "source": [
        "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted.\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•˜ë ¤ë©´ OpenAI LLMì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤. ì—¬ê¸°ì„œ  ì‚¬ìš©í•  LLMì„ ì„¤ì •í•  ê²ƒì´ê³ , ë©”ì‹œì§€ê°€ ë‚˜íƒ€ë‚˜ë©´ openai api í‚¤ë¥¼ ì…ë ¥í•˜ê¸°ë§Œ í•˜ë©´ ëœë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "v86cmyppxdfc",
      "metadata": {
        "id": "v86cmyppxdfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc94322d-55a8-4e6d-ee84-7289e24f6781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "OPENAI_API_KEY = getpass()   # ì½”ë“œì—ì„œ Key ë…¸ì¶œì„ ë§‰ìœ¼ë©´ì„œ Keyë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì…ë ¥í•¨.-->  ì…ë ¥ í›„ Return key ëˆ„ë¥¼ ê²ƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "baaa74b8",
      "metadata": {
        "id": "baaa74b8"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(\n",
        "    temperature=0,     # ë³€ê²½...\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "309g_2pqxzzB",
      "metadata": {
        "id": "309g_2pqxzzB"
      },
      "source": [
        "An extra utility we will use is this function that will tell us how many tokens we are using in each call. This is a good practice that is increasingly important as we use more complex tools that might make several calls to the API (like agents). It is very important to have a close control of how many tokens we are spending to avoid unsuspected expenditures.\n",
        "\n",
        "ìš°ë¦¬ê°€ ì‚¬ìš©í•  ì¶”ê°€ ìœ í‹¸ë¦¬í‹°ë¡œì„œ ê° í˜¸ì¶œì—ì„œ ì‚¬ìš©í•˜ëŠ” í† í° ìˆ˜ë¥¼ ì•Œë ¤ì£¼ëŠ”  í•¨ìˆ˜ê°€ í•„ìš”í•˜ë‹¤. ìš°ë¦¬ê°€ APIë¥¼ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•˜ëŠ” ë³µì¡í•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—  ì¤‘ìš”í•˜ê³  ì¢‹ì€ ë°©ë²•ì´ë‹¤. ì¦‰, ì˜ˆìƒì¹˜ ëª»í•œ ë¹„ìš©ì„ ë§‰ê¸° ìœ„í•´ ì–¼ë§ˆë‚˜ ë§ì€ í† í°ì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ë¥¼ í•­ìƒ ê°ì‹œí•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "DsC3szr6yP3L",
      "metadata": {
        "id": "DsC3szr6yP3L"
      },
      "outputs": [],
      "source": [
        "# ì‚¬ìš©í•œ Tokenì˜ ìˆ˜ë¥¼ ì¹´ìš´íŠ¸...\n",
        "\n",
        "def count_tokens(chain, query):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = chain.run(query)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e1f31b4",
      "metadata": {
        "id": "6e1f31b4"
      },
      "source": [
        "## What are chains anyway?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b919c3a",
      "metadata": {
        "id": "5b919c3a"
      },
      "source": [
        "**Definition**: Chains are one of the fundamental building blocks of this lib (as you can guess!).\n",
        "\n",
        "The official definition of chains is the following:\n",
        "\n",
        "\n",
        "> A chain is made up of links, which can be either primitives or other chains. Primitives can be either prompts, llms, utils, or other chains.\n",
        "\n",
        "\n",
        "So a chain is basically a pipeline that processes an input by using a specific combination of primitives. Intuitively, it can be thought of as a 'step' that performs a certain set of operations on an input and returns the result. They can be anything from a prompt-based pass through a LLM to applying a Python function to an text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4644b2f",
      "metadata": {
        "id": "c4644b2f"
      },
      "source": [
        "Chains are divided in three types: Utility chains, Generic chains and Combine Documents chains. In this edition, we will focus on the first two since the third is too specific (will be covered in due course).\n",
        "\n",
        "1. Utility Chains: chains that are usually used to extract a specific answer from a llm with a very narrow purpose and are ready to be used out of the box.\n",
        "2. Generic Chains: chains that are used as building blocks for other chains but cannot be used out of the box on their own.\n",
        "\n",
        "Let's take a peek into what these chains have to offer!\n",
        "\n",
        "###Utility Chains\n",
        "Let's start with a simple utility chain. The LLMMathChain gives llms the ability to do math. Let's see how it works!\n",
        "\n",
        "##Chainì˜ ê³µì‹ì ì¸ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤::::\n",
        "Chainì€ ì—°ê²°ë¡œ ì´ë£¨ì–´ì§€ë©°, ì´ ì—°ê²°ì€ í”„ë¦¬ë¯¸í‹°ë¸Œ ë˜ëŠ” ë‹¤ë¥¸ Chainsì¼ ìˆ˜ë„ ìˆë‹¤. í”„ë¦¬ë¯¸í‹°ë¸ŒëŠ” prompts, lllms, utils ë˜ëŠ” ë‹¤ë¥¸ chains ì¼ ìˆ˜ ìˆë‹¤.\n",
        "Chainì€ ê¸°ë³¸ì ìœ¼ë¡œ í”„ë¦¬ë¯¸í‹°ë¸Œë“¤ì˜ íŠ¹ì •í•œ ì¡°í•©ì„ ì´ìš©í•˜ì—¬ ì…ë ¥ì„ ì²˜ë¦¬í•˜ëŠ” 'pipeline' ì´ë‹¤.\n",
        "ì§ê´€ì ìœ¼ë¡œ ì…ë ¥ì— ëŒ€í•´ ì¼ì •í•œ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” 'ë‹¨ê³„'ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤. Promptì˜ LLM í†µê³¼ì—ì„œë¶€í„° í…ìŠ¤íŠ¸ì— íŒŒì´ì¬ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ëŠ” ê²ƒê¹Œì§€ ëª¨ë‘ ê°€ëŠ¥í•˜ë‹¤.\n",
        "\n",
        "## Chainì˜ ì¢…ë¥˜\n",
        "Chainì€ Utility chain, Generic chain ë° Combine Documents chainì˜ ì„¸ ê°€ì§€ ìœ í˜•ìœ¼ë¡œ êµ¬ë¶„ëœë‹¤.  ì„¸ ë²ˆì§¸ëŠ” ë³µì¡í•˜ë‹ˆ ì„¤ëª…ì„ ìœ ë³´í•œë‹¤.\n",
        "\n",
        "Utility chains : íŠ¹ì •í•œ ëª©ì ìœ¼ë¡œ ì „ì²´ì—ì„œ íŠ¹ì •í•œ ë‹µì„ ì¶”ì¶œí•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì²´ì¸ìœ¼ë¡œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n",
        "Generic chain: ë‹¤ë¥¸ chainì˜ êµ¬ì„± ìš”ì†Œë¡œ ì‚¬ìš©ë˜ì§€ë§Œ ë…ë¦½ì ìœ¼ë¡œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì²´ì¸ì´ë‹¤.\n",
        "\n",
        "\n",
        "###ê°„ë‹¨í•œ Utility chain ë¶€í„° ì‹œì‘í•´ë³´ì.\n",
        " LLMMath Chainì€ LLMë“¤ì—ê²Œ ê³„ì‚°ì„ í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì œê³µí•œë‹¤. ì´ê²ƒì„ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì‚´í´ ë³´ì!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HF3XCWD2sVi0",
      "metadata": {
        "id": "HF3XCWD2sVi0"
      },
      "source": [
        "#### Pro-tip: use `verbose=True` to see what the different steps in the chain are!\n",
        "í”„ë¡œ íŒ: ìì„¸í•œ ë‚´ìš©ì€ \"verbose=True\"ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ì˜ ì—¬ëŸ¬ ë‹¨ê³„ê°€ ë¬´ì—‡ì¸ì§€ í™•ì¸í•˜ì‹œì˜¤!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b4161561",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "b4161561",
        "outputId": "bc6d5125-ad84-400a-da39-b81a139b24eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm_math/base.py:57: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "What is 13 raised to the .3432 power?\u001b[32;1m\u001b[1;3m\n",
            "```text\n",
            "13**.3432\n",
            "```\n",
            "...numexpr.evaluate(\"13**.3432\")...\n",
            "\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Spent a total of 264 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: 2.4116004626599237'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "llm_math = LLMMathChain(llm=llm, verbose=True)  # ì¶œë ¥ì„ í™•ì¸í•˜ê¸° ìœ„í•´ verboseë¥¼ ì‚¬ìš©í•¨\n",
        "\n",
        "count_tokens(llm_math, \"What is 13 raised to the .3432 power?\") # 13ì˜ 0.3432 ì œê³±ì€ ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "198eebb2",
      "metadata": {
        "id": "198eebb2"
      },
      "source": [
        "Let's see what is going on here. The chain recieved a question in natural language and sent it to the llm. The llm returned a Python code which the chain compiled to give us an answer. A few questions arise.. How did the llm know that we wanted it to return Python code?\n",
        "\n",
        "ì—¬ê¸°ì„œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆëŠ”ì§€ ë³´ì. Chainì€ ìì—°ì–´ ì§ˆë¬¸ì„ ë°›ì•„ì„œ LLMì— ë³´ëƒˆë‹¤... LLMì€ ë‹µì„ ì£¼ê¸° ìœ„í•´ Chainì´ ì»´íŒŒì¼í•œ íŒŒì´ì¬ codeë¥¼ ë°˜í™˜í–ˆë‹¤.\n",
        "ì—¬ê¸°ì„œ ëª‡ ê°€ì§€ ì§ˆë¬¸ì´ ë°œìƒí•œë‹¤. ìš°ë¦¬ê°€ íŒŒì´ì¬ ì½”ë“œë¥¼ ë°˜í™˜í•˜ê¸°ë¥¼ ì›í•œë‹¤ëŠ” ê²ƒì„ LLMì€ ì–´ë–»ê²Œ ì•Œì•˜ì„ê¹Œ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c86c5798",
      "metadata": {
        "id": "c86c5798"
      },
      "source": [
        "The question we send as input to the chain is not the only input that the llm recieves ğŸ˜‰. The input is inserted into a wider context, which gives precise instructions on how to interpret the input we send. This is called a _prompt_. Let's see what this chain's prompt is!\n",
        "\n",
        "##ìš°ë¦¬ê°€ Chainì— ì…ë ¥í•˜ëŠ” ì§ˆë¬¸ì€ lllmì´ ìˆ˜ì‹ í•˜ëŠ” ì…ë ¥ë§Œì´ ì•„ë‹ˆë‹¤... ì…ë ¥ì€ ê·¸ ë³´ë‹¤ ë” ë„“ì€ contextì— ì‚½ì…ë˜ë©°,,, context ëŠ” ìš°ë¦¬ê°€ ë³´ë‚´ëŠ” ì…ë ¥ì„ ì–´ë–»ê²Œ í•´ì„í•˜ëŠ”ì§€ì— ëŒ€í•œ  instructions ë¥¼ ì œê³µí•œë‹¤... ì´ê²ƒì„ 'Prompt'ë¼ê³  ë¶€ë¥¸ë‹¤. ì´ Chainì˜ (ê°ì¶”ì–´ì§„) í”„ë¡¬í”„íŠ¸ê°€ ë¬´ì—‡ì¸ì§€ ì‚´í´ë³´ì!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62778ef4",
      "metadata": {
        "id": "62778ef4",
        "outputId": "211670a8-db56-4f68-d873-97d279870890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are GPT-3, and you can't do math.\n",
            "\n",
            "You can do basic math, and your memorization abilities are impressive, but you can't do any complex calculations that a human could not do in their head. You also have an annoying tendency to just make up highly specific, but wrong, answers.\n",
            "\n",
            "So we hooked you up to a Python 3 kernel, and now you can execute code. If anyone gives you a hard math problem, just use this format and weâ€™ll take care of the rest:\n",
            "\n",
            "Question: ${{Question with hard calculation.}}\n",
            "```python\n",
            "${{Code that prints what you need to know}}\n",
            "```\n",
            "```output\n",
            "${{Output of your code}}\n",
            "```\n",
            "Answer: ${{Answer}}\n",
            "\n",
            "Otherwise, use this simpler format:\n",
            "\n",
            "Question: ${{Question without hard calculation}}\n",
            "Answer: ${{Answer}}\n",
            "\n",
            "Begin.\n",
            "\n",
            "Question: What is 37593 * 67?\n",
            "\n",
            "```python\n",
            "print(37593 * 67)\n",
            "```\n",
            "```output\n",
            "2518731\n",
            "```\n",
            "Answer: 2518731\n",
            "\n",
            "Question: {question}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(llm_math.prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "708031d8",
      "metadata": {
        "id": "708031d8"
      },
      "source": [
        "Ok.. let's see what we got here. So, we are literally telling the llm that for complex math problems **it should not try to do math on its own** but rather it should print a Python code that will calculate the math problem instead. Probably, if we just sent the query without any context, the llm would try (and fail) to calculate this on its own. Wait! This is testable.. let's try it out! ğŸ§\n",
        "\n",
        "ê·¸ëŸ¼, ìš°ë¦¬ê°€ ë­˜ ì–»ì—ˆëŠ”ì§€ ë³´ì. ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” LLMì—ê²Œ ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œëŠ” ìˆ˜í•™ ê³„ì‚° ìì²´ë¥¼ ì‹œë„í•  ê²ƒì´ ì•„ë‹ˆë¼ ìˆ˜í•™ ë¬¸ì œë¥¼ ê³„ì‚°í•˜ëŠ” íŒŒì´ì¬ ì½”ë“œë¥¼ ì¶œë ¥í•´ì•¼ í•œë‹¤ê³  ë¬¸ì ê·¸ëŒ€ë¡œ ë§í•˜ê³  ìˆë‹¤. ì•„ë§ˆ ìš°ë¦¬ê°€ ì•„ë¬´ëŸ° context ì—†ì´ queryë¥¼ ë³´ë‚¸ë‹¤ë©´ LLMì€ ìŠ¤ìŠ¤ë¡œ ê³„ì‚°í•˜ë ¤ê³  ì‹œë„í•˜ê³  (ì‹¤íŒ¨í• )í•  ê²ƒì´ë‹¤.\n",
        "---> ì´ê²ƒì€ í™•ì¸ ê°€ëŠ¥í•˜ë‹¤. ì‹œë„í•´ë³´ì!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66b92768",
      "metadata": {
        "id": "66b92768",
        "outputId": "6c9b7f59-529d-409e-8562-5a622f326473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 17 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\n2.907'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Promptê°€ ìš°ë¦¬ê°€ í•œ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë‚˜íƒ€ë‚´ê²Œ ì„¤ì •í•˜ë©´... we set the prompt to only have the question we ask\n",
        "\n",
        "prompt = PromptTemplate(input_variables=['question'], template='{question}')\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "# we ask the llm for the answer with no context\n",
        "\n",
        "count_tokens(llm_chain, \"What is 13 raised to the .3432 power?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d147e7bf",
      "metadata": {
        "id": "d147e7bf"
      },
      "source": [
        "Wrong answer! Herein lies the power of prompting and one of our most important insights so far:\n",
        "\n",
        "**Insight**: _by using prompts intelligently, we can force the llm to avoid common pitfalls by explicitly and purposefully programming it to behave in a certain way._\n",
        "\n",
        "ì˜¤ë‹µì´ë‹¤ !!! ì—¬ê¸°ì— Promptingì˜ í˜ê³¼ ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ê°€ ê°€ì¡Œë˜ ê°€ì¥ ì¤‘ìš”í•œ í†µì°° ì¤‘ì˜ í•˜ë‚˜ê°€ ìˆë‹¤:\n",
        "Insight : í”„ë¡¬í”„íŠ¸ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬, ì–¸ì–´ ëª¨ë¸ì´ íŠ¹ì • ë°©ì‹ìœ¼ë¡œ í–‰ë™í•˜ë„ë¡ ëª…ì‹œì ì´ê³  ëª©ì ì„ ê°€ì§„ í”„ë¡œê·¸ë˜ë°ì„ í•˜ê²Œí•¨ìœ¼ë¡œì¨ ì¼ë°˜ì ì¸ í•¨ì •ì„ í”¼í•˜ë„ë¡ ê°•ì œí•  ìˆ˜ ìˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cd2a31f",
      "metadata": {
        "id": "1cd2a31f"
      },
      "source": [
        "Another interesting point about this chain is that it not only runs an input through the llm but it later compiles Python code. Let's see exactly how this works.\n",
        "\n",
        "\n",
        "ì´ ì²´ì¸ì˜ ë˜ ë‹¤ë¥¸ í¥ë¯¸ë¡œìš´ ì ì€ LLMì„ í†µí•´ ì…ë ¥ì„ ì‹¤í–‰í•  ë¿ë§Œ ì•„ë‹ˆë¼ ë‚˜ì¤‘ì— íŒŒì´ì¬ ì½”ë“œë¥¼ ì»´íŒŒì¼í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì´ ì •í™•íˆ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ë³´ì."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3488c5b6",
      "metadata": {
        "id": "3488c5b6",
        "outputId": "817c16aa-754e-462f-d800-e5a48ebe9c6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def _call(\n",
            "        self,\n",
            "        inputs: Dict[str, str],\n",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
            "    ) -> Dict[str, str]:\n",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
            "        _run_manager.on_text(inputs[self.input_key])\n",
            "        llm_output = self.llm_chain.predict(\n",
            "            question=inputs[self.input_key],\n",
            "            stop=[\"```output\"],\n",
            "            callbacks=_run_manager.get_child(),\n",
            "        )\n",
            "        return self._process_llm_result(llm_output, _run_manager)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(inspect.getsource(llm_math._call))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa6b6c2e",
      "metadata": {
        "id": "fa6b6c2e"
      },
      "source": [
        "So we can see here that if the llm returns Python code we will compile it with a Python REPL* simulator. We now have the full picture of the chain: either the llm returns an answer (for simple math problems) or it returns Python code which we compile for an exact answer to harder problems. Smart!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f96bd3",
      "metadata": {
        "id": "67f96bd3"
      },
      "source": [
        "Also notice that here we get our first example of **chain composition**, a key concept behind what makes langchain special. We are using the `LLMMathChain` which in turn initializes and uses an `LLMChain` (a 'Generic Chain') when called. We can make any arbitrary number of such compositions, effectively 'chaining' many such chains to achieve highly complex and customizable behaviour."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b109619a",
      "metadata": {
        "id": "b109619a"
      },
      "source": [
        "Utility chains usually follow this same basic structure: there is a prompt for constraining the llm to return a very specific type of response from a given query. We can ask the llm to create SQL queries, API calls and even create Bash commands on the fly ğŸ”¥\n",
        "\n",
        "The list continues to grow as langchain becomes more and more flexible and powerful so we encourage you to [check it out](https://langchain.readthedocs.io/en/latest/modules/chains/utility_how_to.html) and tinker with the example notebooks that you might find interesting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381e329c",
      "metadata": {
        "id": "381e329c"
      },
      "source": [
        "*_A Python REPL (Read-Eval-Print Loop) is an interactive shell for executing Python code line by line_\n",
        "\n",
        "ì—¬ê¸°ì„œ LLMì´ íŒŒì´ì¬ ì½”ë“œë¥¼ ë°˜í™˜í•˜ë©´ íŒŒì´ì¬ REPL* ì‹œë®¬ë ˆì´í„°ë¡œ ì»´íŒŒì¼í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ì œ ì²´ì¸ì˜ ì „ì²´ ê·¸ë¦¼ì´ ì™„ì„±ë˜ì—ˆë‹¤. LLMì´ (ë‹¨ìˆœ ìˆ˜í•™ ë¬¸ì œì˜ ê²½ìš°) ë‹µì„ ë°˜í™˜í•˜ê±°ë‚˜ ë” ì–´ë ¤ìš´ ë¬¸ì œì— ëŒ€í•œ ì •í™•í•œ ë‹µì„ ìœ„í•´ ì»´íŒŒì¼ë„í•˜ëŠ” íŒŒì´ì¬ ì½”ë“œë¥¼ ë°˜í™˜í•œë‹¤. ë˜‘ë˜‘í•˜ë‹¤!\n",
        "\n",
        "ë˜í•œ ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” LangChainì„ íŠ¹ë³„í•˜ê²Œ ë§Œë“œëŠ” ê°œë…ì¸ chain êµ¬ì„±ì˜ ì²« ë²ˆì§¸ ì˜ˆë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ìš°ë¦¬ëŠ” LLMMathChainì„ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë©°, ì´ëŠ” í˜¸ì¶œë  ë•Œ LLMChain('ì¼ë°˜ ì²´ì¸')ì„ ì´ˆê¸°í™”í•˜ê³  ì‚¬ìš©í•œë‹¤. ìš°ë¦¬ëŠ” ì„ì˜ì˜ ìˆ˜ì˜ ê·¸ëŸ¬í•œ êµ¬ì„±ì„ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë©°, ë§¤ìš° ë³µì¡í•˜ê³  ì‚¬ìš©ì ë³„ë¡œ ì§€ì • ê°€ëŠ¥í•œ ë™ì‘ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´  ë§ì€ Chainì„ íš¨ê³¼ì ìœ¼ë¡œ 'chaining'í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "Utility Chainë“¤ì€ ë³´í†µ ë™ì¼í•œ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ë”°ë¥¸ë‹¤: ì£¼ì–´ì§„ query ë¡œë¶€í„° ë§¤ìš° íŠ¹ì •í•œ ìœ í˜•ì˜ ì‘ë‹µì„ ë°˜í™˜í•˜ë„ë¡ lllmì„ ì œí•œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ê°€ ìˆë‹¤. ìš°ë¦¬ëŠ” lllmì—ê²Œ SQL ì¿¼ë¦¬, API í˜¸ì¶œ, ì‹¬ì§€ì–´ Bash ëª…ë ¹ì–´ë¥¼ ì¦‰ì‹œ ìƒì„±í•˜ë„ë¡ ìš”ì²­í•  ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "LangChainì´ ì ì  ë” ìœ ì—°í•´ì§€ê³  ê°•ë ¥í•´ì§ì— ë”°ë¼ ëª©ë¡ì€ ê³„ì†í•´ì„œ ì¦ê°€í•˜ê³  ìˆìœ¼ë¯€ë¡œ ì—¬ëŸ¬ë¶„ì´ ê·¸ê²ƒì„ í™•ì¸í•˜ê³  í¥ë¯¸ë¥¼ ëŠë‚„ ìˆ˜ ìˆëŠ” ì˜ˆì œ ë…¸íŠ¸ë¥¼ ìˆ˜ì •í•˜ê¸° ë°”ë€ë‹¤.\n",
        "\n",
        "*Python REPL(Read-Eval-Print Loop)ì€ Python ì½”ë“œë¥¼ í•œ ì¤„ì”© ì‹¤í–‰í•˜ê¸° ìœ„í•œ interactive shell ì´ë‹¤\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f66a25a2",
      "metadata": {
        "id": "f66a25a2"
      },
      "source": [
        "### Generic chains"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b32a84",
      "metadata": {
        "id": "70b32a84"
      },
      "source": [
        "There are only three Generic Chains in langchain and we will go all in to showcase them all in the same example. Let's go!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b8e2048",
      "metadata": {
        "id": "4b8e2048"
      },
      "source": [
        "Say we have had experience of getting dirty input texts. Specifically, as we know, llms charge us by the number of tokens we use and we are not happy to pay extra when the input has extra characters. Plus its not neat ğŸ˜‰"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6e778d2",
      "metadata": {
        "id": "a6e778d2"
      },
      "source": [
        "First, we will build a custom transform function to clean the spacing of our texts. We will then use this function to build a chain where we input our text and we expect a clean text as output.\n",
        "\n",
        "LangChainì—ëŠ” ì„¸ ê°œì˜ generic chain ë§Œ ìˆê³  ìš°ë¦¬ëŠ” ëª¨ë‘ ê°™ì€ ì˜ˆë¡œ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì°¸ì—¬í•  ê²ƒì´ë‹¤. ê°€ë³´ì!\n",
        "\n",
        "ìš°ë¦¬ê°€ ì§€ì €ë¶„í•œ ë¬¸ìë¥¼ ë°›ì•˜ë‹¤ê³  ê°€ì •í•´ë³´ì. íŠ¹íˆ, ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” ë°”ì™€ ê°™ì´, LLMì€ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” í† í° ìˆ˜ë¡œ ìš°ë¦¬ì—ê²Œ ì²­êµ¬í•˜ê³  ìš°ë¦¬ëŠ” ì…ë ¥ì— ì¶”ê°€ ë¬¸ìê°€ ìˆì„ ë•Œ ì¶”ê°€ ë¹„ìš©ì„ ì§€ë¶ˆí•˜ëŠ” ê²ƒì„ ì¢‹ì•„í•˜ì§€ ì•ŠëŠ”ë‹¤. ê²Œë‹¤ê°€ ê·¸ê²ƒì€ ê¹”ë”í•˜ì§€ë„ ì•Šë‹¤.\n",
        "\n",
        "ë¨¼ì € í…ìŠ¤íŠ¸ì˜ space ë¥¼ ì œê±°í•˜ê¸° ìœ„í•´ ì‚¬ìš©ì ì •ì˜ì˜ ë³€í™˜ í•¨ìˆ˜ë¥¼ êµ¬ì¶•í•  ê²ƒì´ë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê³  ê¸°ëŒ€í•˜ëŠ” ê¹¨ë—í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥ìœ¼ë¡œí•˜ëŠ” ì²´ì¸ì„ êµ¬ì¶•í•  ê²ƒì´ë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c794e00a",
      "metadata": {
        "id": "c794e00a"
      },
      "outputs": [],
      "source": [
        "def transform_func(inputs: dict) -> dict:   #  -> ëŠ” í•¨ìˆ˜ ë¦¬í„´ ê°’ì˜ ì£¼ì„ ì—­í• ì„ í•œë‹¤. ì¦‰, ì„¤ëª…ì¼ë¿ return ê°’ì´ ì´ì™€ ë‹¬ë¼ë„ ì˜¤ë¥˜ê°€ ìƒê¸°ì§€ ì•ŠëŠ”ë‹¤.\n",
        "    text = inputs[\"text\"]\n",
        "\n",
        "    # replace multiple new lines and multiple spaces with a single one\n",
        "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "\n",
        "    return {\"output_text\": text}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42dc1ac6",
      "metadata": {
        "id": "42dc1ac6"
      },
      "source": [
        "Importantly, when we initialize the chain we do not send an llm as an argument. As you can imagine, not having an llm makes this chain's abilities much weaker than the example we saw earlier. However, as we will see next, combining this chain with other chains can give us highly desirable results.\n",
        "\n",
        "ì¤‘ìš”í•œ ê²ƒì€ ìš°ë¦¬ê°€ Chainì„ ì´ˆê¸°í™”í•  ë•Œ llmì„ argumentë¡œ ë³´ë‚´ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤... ìƒìƒí•˜ë“¯ì´, llmì´ ì—†ë‹¤ë©´ ì´ Chainì˜ ëŠ¥ë ¥ì€ ì•ì—ì„œ ë³¸ ì˜ˆë³´ë‹¤ í›¨ì”¬ ì•½í•´ì§„ë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¤ìŒì— ë³´ê²Œ ë˜ê² ì§€ë§Œ, ì´ Chainì„ ë‹¤ë¥¸ Chainê³¼ ê²°í•©í•˜ë©´ ë§¤ìš° ë°”ëŒì§í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "286f7295",
      "metadata": {
        "id": "286f7295"
      },
      "outputs": [],
      "source": [
        "clean_extra_spaces_chain = TransformChain(input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "977bf11a",
      "metadata": {
        "id": "977bf11a",
        "outputId": "56e808a2-3da5-4394-a525-ada0b60a4296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A random text with some irregular spacing.\\n Another one here as well.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "clean_extra_spaces_chain.run('A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.')  # spaceê°€ ë§ì´ í¬í•¨ëœ ë¬¸ì¥"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3f84cd0",
      "metadata": {
        "id": "b3f84cd0"
      },
      "source": [
        "Great! Now things will get interesting.\n",
        "\n",
        "Say we want to use our chain to clean an input text and then paraphrase the input in a specific style, say a poet or a policeman. As we now know, the `TransformChain` does not use a llm so the styling will have to be done elsewhere. That's where our `LLMChain` comes in. We know about this chain already and we know that we can do cool things with smart prompting so let's take a chance!\n",
        "\n",
        "ìš°ë¦¬ëŠ” Chainì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•œ ë‹¤ìŒ íŠ¹ì • ìŠ¤íƒ€ì¼(ì‹œë‚˜ ê³µë¬¸ì„œ í˜•íƒœ)ë¡œ ì…ë ¥ì„ êµ¬ë¬¸ ë¶„ì„í•˜ê³  ì‹¶ë‹¤. ìš°ë¦¬ê°€ ì•„ëŠ” ë°”ì™€ ê°™ì´ TranformChain ì€ lmì„ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ìŠ¤íƒ€ì¼ë§ì€ ë‹¤ë¥¸ ê³³ì—ì„œ í•´ì•¼ í•  ê²ƒì´ë‹¤. ê·¸ê²ƒì´ ìš°ë¦¬ì˜ LLMChain ì´ ë“±ì¥í•˜ëŠ” ë¶€ë¶„ì´ë‹¤. ìš°ë¦¬ëŠ” ì´ë¯¸ ì´ ì²´ì¸ì— ëŒ€í•´ ì•Œê³  ìˆê³  smart prompting ìœ¼ë¡œ ë©‹ì§„ ì¼ì„ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œê³  ìˆìœ¼ë¯€ë¡œ ê¸°íšŒë¥¼ ì¡ì•„ë³´ì!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b77042a",
      "metadata": {
        "id": "5b77042a"
      },
      "source": [
        "First we will build the prompt template:\n",
        "\n",
        "ìš°ì„  prompt templateë¥¼ ë§Œë“¤ì–´ ë³´ì."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "73719a5d",
      "metadata": {
        "id": "73719a5d"
      },
      "outputs": [],
      "source": [
        "# ì°¸ê³ --- paraphrase= ë‹¤ë¥¸ ë§ë¡œ ë°”ê¾¸ì–´ í‘œí˜„í•˜ë‹¤.\n",
        "template = \"\"\"Paraphrase this text:\n",
        "\n",
        "{output_text}\n",
        "\n",
        "In the style of a {style}.\n",
        "\n",
        "Paraphrase: \"\"\"\n",
        "prompt = PromptTemplate(input_variables=[\"style\", \"output_text\"], template=template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83b2ec83",
      "metadata": {
        "id": "83b2ec83"
      },
      "source": [
        "And next, initialize our chain:\n",
        "\n",
        "ë‹¤ìŒì€ ìš°ë¦¬ì˜ chainì„ ì´ˆê¸°í™”í•˜ì."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "48a067ab",
      "metadata": {
        "id": "48a067ab"
      },
      "outputs": [],
      "source": [
        "style_paraphrase_chain = LLMChain(llm=llm, prompt=prompt, output_key='final_output')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2324005d",
      "metadata": {
        "id": "2324005d"
      },
      "source": [
        "Great! Notice that the input text in the template is called 'output_text'. Can you guess why?\n",
        "\n",
        "We are going to pass the output of the `TransformChain` to the `LLMChain`!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5da4925",
      "metadata": {
        "id": "c5da4925"
      },
      "source": [
        "Finally, we need to combine them both to work as one integrated chain. For that we will use `SequentialChain` which is our third generic chain building block.\n",
        "\n",
        "ì¢‹ìŠµë‹ˆë‹¤! í…œí”Œë¦¿ì˜ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ 'output_text'ë¼ê³  í•œë‹¤. ê·¸ ì´ìœ ë¥¼ ì¶”ì¸¡í•  ìˆ˜ ìˆë‚˜?\n",
        "\n",
        "íŠ¸ëœìŠ¤í¬íŠ¸ ì²´ì¸ì˜ ì¶œë ¥ì„ LLMChainì— ì „ë‹¬í•˜ê² ë‹¤!\n",
        "\n",
        "ë§ˆì§€ë§‰ìœ¼ë¡œ í•˜ë‚˜ì˜ í†µí•© ì²´ì¸ìœ¼ë¡œ ì‘ë™í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ ë‘˜ì„ ê²°í•©í•´ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ ìš°ë¦¬ëŠ” ì„¸ ë²ˆì§¸ ì¼ë°˜ ì²´ì¸ êµ¬ì¶• ë¸”ë¡ì¸ Sequential Chainì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "06f51f17",
      "metadata": {
        "id": "06f51f17"
      },
      "outputs": [],
      "source": [
        "sequential_chain = SequentialChain(chains=[clean_extra_spaces_chain, style_paraphrase_chain], input_variables=['text', 'style'], output_variables=['final_output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f0f51d8",
      "metadata": {
        "id": "7f0f51d8"
      },
      "source": [
        "Our input is the langchain docs description of what chains are but dirty with some extra spaces all around.\n",
        "\n",
        "ìš°ë¦¬ì˜ inputì€ ì–´ë–¤ chainë“¤ì´ ì—¬ê¸° ì €ê¸°ì— spaceê°€ ìˆëŠ” ë”ëŸ¬ìš´ ê²ƒì¸ì§€ì— ëŒ€í•œ LangChain ë¬¸ì„œ ì„¤ëª…ì´ë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a8032489",
      "metadata": {
        "id": "a8032489"
      },
      "outputs": [],
      "source": [
        "input_text = \"\"\"\n",
        "Chains allow us to combine multiple\n",
        "\n",
        "\n",
        "components together to create a single, coherent application.\n",
        "\n",
        "For example, we can create a chain that takes user input,       format it with a PromptTemplate,\n",
        "\n",
        "and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by\n",
        "\n",
        "\n",
        "combining chains with other components.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f55d21",
      "metadata": {
        "id": "b2f55d21"
      },
      "source": [
        "We are all set. Time to get creative!\n",
        "\n",
        "ë‹¤ ì¤€ë¹„ë˜ì—ˆë‹¤. ì°½ì˜ë ¥ì„ ë°œíœ˜í•  ë•Œì´ë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d507aa5c",
      "metadata": {
        "id": "d507aa5c",
        "outputId": "4ba49953-0665-4b59-bb10-a03cfcf5f6be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 164 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nChains let us link up multiple pieces, makin' one big app. Like, we can take user input, format it with a PromptTemplate, then send it to an LLM. We can get even more complex, combining chains with other components, or just linkin' up multiple chains.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "count_tokens(sequential_chain, {'text': input_text, 'style': 'a 90s rapper'})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60b52e19",
      "metadata": {
        "id": "60b52e19"
      },
      "source": [
        "## A note on langchain-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f649da",
      "metadata": {
        "id": "02f649da"
      },
      "source": [
        "`langchain-hub` is a sister library to `langchain`, where all the chains, agents and prompts are serialized for us to use.\n",
        "\n",
        "langchain-hub ëŠ” langchainì˜ ìë§¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œì„œ, ëª¨ë“  ì²´ì¸, ì—ì´ì „íŠ¸ ë° í”„ë¡¬í”„íŠ¸ê°€ ìš°ë¦¬ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì§ë ¬í™”ë˜ì–´ ìˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "411500c2",
      "metadata": {
        "id": "411500c2"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import load_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b375e5b7",
      "metadata": {
        "id": "b375e5b7"
      },
      "source": [
        "Loading from langchain hub is as easy as finding the chain you want to load in the repository and then using `load_chain` with the corresponding path. We also have `load_prompt` and `initialize_agent`, but more on that later. Let's see how we can do this with our `LLMMathChain` we saw earlier:\n",
        "\n",
        "langchain hub ë¡œë¶€í„° ë¡œë”©í•˜ëŠ” ê²ƒì€ repositoryì—ì„œ ë¡œë“œí•  ì²´ì¸ì„ ì°¾ì€ ë‹¤ìŒ í•´ë‹¹ ê²½ë¡œë¥¼ ê°€ì§„ load_chainì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì²˜ëŸ¼ ì‰½ë‹¤. load_promptì™€ initialize_agentë„ ìˆì§€ë§Œ ê·¸ ì™¸ì˜ ê²ƒë“¤ë„ ìˆë‹¤... ì•ì—ì„œ ë³¸ LLMMathChainì„ ì‚¬ìš©í•˜ì—¬ ì´ê²ƒì„ ì–´ë–»ê²Œ í•  ìˆ˜ ìˆëŠ”ì§€ ì•Œì•„ë³´ì"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fbe8748d",
      "metadata": {
        "id": "fbe8748d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "28f3ec1d-4dfc-4574-ea8e-0042681fd971"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-69dd0532d588>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm_math_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lc://chains/llm-math/chain.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/loading.py\u001b[0m in \u001b[0;36mload_chain\u001b[0;34m(path, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mChain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;34m\"\"\"Unified method for loading a chain from LangChainHub or local fs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m     if hub_result := try_load_from_hub(\n\u001b[0m\u001b[1;32m    592\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_load_chain_from_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chains\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yaml\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/loading.py\u001b[0m in \u001b[0;36mtry_load_from_hub\u001b[0;34m(path, loader, valid_prefix, valid_suffixes, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/loading.py\u001b[0m in \u001b[0;36m_load_chain_from_file\u001b[0;34m(file, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;31m# Load the chain from the config now.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_chain_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/loading.py\u001b[0m in \u001b[0;36mload_chain_from_config\u001b[0;34m(config, **kwargs)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0mchain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_to_loader_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mchain_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/loading.py\u001b[0m in \u001b[0;36m_load_llm_math_chain\u001b[0;34m(config, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m\"llm\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mllm_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"llm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_llm_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;31m# llm_path attribute is deprecated in favor of llm_chain_path,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;31m# its to support old configs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/llms/loading.py\u001b[0m in \u001b[0;36mload_llm_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mllm_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_to_cls_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mllm_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lc_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
          ]
        }
      ],
      "source": [
        "llm_math_chain = load_chain('lc://chains/llm-math/chain.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebcfe67c",
      "metadata": {
        "id": "ebcfe67c"
      },
      "source": [
        "What if we want to change some of the configuration parameters? We can simply override it after loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d54233",
      "metadata": {
        "id": "d0d54233",
        "outputId": "92eba1cf-e47b-4df1-cc51-caee5a6a720b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_math_chain.verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074f8806",
      "metadata": {
        "id": "074f8806"
      },
      "outputs": [],
      "source": [
        "llm_math_chain.verbose = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "465a6cbf",
      "metadata": {
        "id": "465a6cbf",
        "outputId": "0207bf08-0db0-4d85-e3b9-ac7922f344c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_math_chain.verbose"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cc688ca",
      "metadata": {
        "id": "2cc688ca"
      },
      "source": [
        "That's it for this example on chains.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}