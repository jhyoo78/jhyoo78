{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM+bYcERJ+DjR78MvD9adgM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhyoo78/jhyoo78/blob/main/synonym_antonym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk  # natural Language toolkit(python)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')    #  Open Multilingual Wordnet collection 1.4\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "BXQ67llsnzdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTISmi0ZnUau"
      },
      "outputs": [],
      "source": [
        "#  참고 -->  https://www.holisticseo.digital/python-seo/nltk/wordnet\n",
        "def synonym_antonym_extractor(phrase):\n",
        "     from nltk.corpus import wordnet\n",
        "     synonyms = []\n",
        "     antonyms = []\n",
        "\n",
        "     for syn in wordnet.synsets(phrase):\n",
        "          for l in syn.lemmas():\n",
        "               synonyms.append(l.name())\n",
        "               if l.antonyms():\n",
        "                    antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "     print(set(synonyms))\n",
        "     print(set(antonyms))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synonym_antonym_extractor(phrase=\"up\")"
      ],
      "metadata": {
        "id": "qJO1L-7CnfY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 유사도 계산하기 ---> https://www.nltk.org/howto/wordnet.html\n",
        "from nltk.corpus import wordnet\n",
        "dog = wordnet.synset('dog.n.01')\n",
        "cat = wordnet.synset('cat.n.01')\n",
        "up = wordnet.synset('up.r.01')    # r= 부사(adverb), a= 형용사(adjective), n=noun, v=verb, s=adjective satellite (antonym이 없는 단어를 의미한다. 특이한 경우인데, wn에서 adjective의 경우는 (adjective, verb, antonym)의 triple의 형태로 관리되는데, anotynym이 없을 경우만 따로 adjective satellite 로 관리한다.)\n",
        "down = wordnet.synset('down.r.01')\n",
        "\n",
        "dog.path_similarity(cat)\n",
        "dog.lch_similarity(cat)\n",
        "#print(wordnet.path_similarity(dog, cat))\n",
        "#print(wordnet.lch_similarity(up, down))   # Leacock-Chodorow Similarity, 이 관계는 -log(p/2d)으로 주어짐. 단, p는 shortest path length 이고 d 는 the taxonomy depth 이다.\n",
        "\n",
        "a1= wordnet.synsets('starting')  # 단어의 원형이 출력됨. 예: 'starting'을 입력하면 'start'가 출력됨\n",
        "a2= wordnet.synsets('started')\n",
        "print(\"a1=\", a1)\n",
        "print(\"a2=\", a2)\n",
        "print(a1[0])\n",
        "b1= wordnet.synset(a1[0].name())\n",
        "b2= wordnet.synset(a2[0].name())\n",
        "print(\"a1[0], a2[0] =\", a1[0], a2[0])\n",
        "\n",
        "# print(wordnet.lch_similarity(b1, b2) )  # 두 단어가 같은 POS 를 가져야 비교 가능함. 즉, 명사와 동사는 유사도 계산 불가함.\n",
        "\n",
        "a1= wordnet.synsets('up', pos='n')        # POS에 해당하는 lemma만 출력\n",
        "a2= wordnet.synsets('down', pos='n')\n",
        "print(\"a1, a2=\", a1, a2)                  # up의 동의어는 없음 --> []으로 출력됨.\n"
      ],
      "metadata": {
        "id": "qadq5OKziox6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 함수 없이 동의어 찾기\n",
        "from nltk.corpus import wordnet\n",
        "synonyms = []\n",
        "\n",
        "print(wordnet.synsets(\"love\"))\n",
        "\n",
        "for syn in wordnet.synsets(\"love\"):\n",
        "    for i in syn.lemmas():\n",
        "        synonyms.append(i.name())\n",
        "\n",
        "print(set(synonyms))\n"
      ],
      "metadata": {
        "id": "iubYMrUgrnKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 반의어 찾기\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "antonyms = []\n",
        "\n",
        "for syn in wordnet.synsets(\"failed\"):\n",
        "    for i in syn.lemmas():\n",
        "         if i.antonyms():\n",
        "              antonyms.append(i.antonyms()[0].name())\n",
        "\n",
        "print(set(antonyms))\n"
      ],
      "metadata": {
        "id": "f4J9cKa-xZb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.holisticseo.digital/python-seo/nltk/wordnet\n",
        "# 문장 내의 단에 대한 동의어 반의어 찾기\n",
        "\n",
        "def text_parser_synonym_antonym_finder(text:str):\n",
        "     from nltk.tokenize import word_tokenize\n",
        "     from nltk.corpus import wordnet\n",
        "     from collections import defaultdict\n",
        "     import pprint\n",
        "\n",
        "     tokens = word_tokenize(text)   # nltk의 tokenizer\n",
        "     synonyms = defaultdict(list)\n",
        "     antonyms = defaultdict(list)\n",
        "     for token in tokens:\n",
        "          for syn in wordnet.synsets(token):\n",
        "               for i in syn.lemmas():\n",
        "                    #synonyms.append(i.name())\n",
        "                    #print(f'{token} synonyms are: {i.name()}')\n",
        "                    synonyms[token].append(i.name())\n",
        "                    if i.antonyms():\n",
        "                         #antonyms.append(i.antonyms()[0].name())\n",
        "                         #print(f'{token} antonyms are: {i.antonyms()[0].name()}')\n",
        "                         antonyms[token].append(i.antonyms()[0].name())\n",
        "     #pprint.pprint(dict(synonyms))   #################### 동의어 출력이 너무 많아서...............반의어만 보기 위해서...\n",
        "     pprint.pprint(dict(antonyms))\n",
        "\n",
        "     '''\n",
        "     synonym_output = pprint.pformat((dict(synonyms)))\n",
        "     antonyms_output = pprint.pformat((dict(antonyms)))\n",
        "     with open(str(text[:5]) + \".txt\", \"a\") as f:\n",
        "          f.write(\"Starting of Synonyms of the Words from the Sentences: \" + synonym_output + \"\\n\")\n",
        "          f.write(\"Starting of Antonyms of the Words from the Sentences: \" + antonyms_output + \"\\n\")\n",
        "          f.close()\n",
        "     '''"
      ],
      "metadata": {
        "id": "KTX-qPk4rnT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')  # 말뭉치 다운로드 필요\n",
        "text_parser_synonym_antonym_finder(text=\"init failed run of stage\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MrDI0u-Wrm36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래는 pretrained Word2Vec 시험 용"
      ],
      "metadata": {
        "id": "XQ7iBAJWOzoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PC 에서 GoogleNews-vectors-negative300.bin.gz 를 upload\n",
        "\n",
        "from google.colab import files     # Colab에서 local PC내의 file 읽어 들이는 라이브러리\n",
        "uploaded = files.upload()          # 파일을  선택"
      ],
      "metadata": {
        "id": "sy0bqRxWOxAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Drive to access data files\n",
        "from google.colab import drive\n",
        "drive.mount('./mount')    # 실행하면 'Colab의 Jupyter에서 Google Drive 파일에 액세스하도록 허용하시겠습니까?' 라는 질문이 나온다. 동의하면 됨\n",
        "# 위 명령에 동의하고 완료되면 Mounted at ./mount 가 출력되고, 좌측에 mount  폴더가 생성된다."
      ],
      "metadata": {
        "id": "NrEzEgOqbXdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "# 우리가 업데이트하려고 하는 data가 다음 sentences에 있다고 합시다\n",
        "# 그리고, 다음 데이터들은 모두 lower, strip, 공백으로 split되어 있다고 하죠.\n",
        "sentences = [\"I am a boy\", \"You are a girl\"]\n",
        "sentences = [s.lower().strip().split(\" \") for s in sentences]\n",
        "#---------------------------------------------\n",
        "# 1) LOAD pre-trained key vector\n",
        "# model을 load한 것이 아니기 때문에, 바로 training하는 것이 불가능하죠.\n",
        "googleNews_filepath = \"GoogleNews-vectors-negative300-SLIM.bin\"\n",
        "PreTrainedKeyvector = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(\n",
        "    googleNews_filepath, binary=True, limit=20\n",
        ")\n",
        "#---------------------------------------------\n",
        "# 2) MAKE new word2vec model\n",
        "# PreTrainedKeyvector와 `vector_size`가 같은 word2vec model을 생성해줌.\n",
        "TranferedModel = gensim.models.Word2Vec(\n",
        "    size=PreTrainedKeyvector.vector_size, min_count=1)\n",
        "#---------------------------------------------\n",
        "# 3) BUILD vocab by PreTrainedKeyvector word Vocabulary\n",
        "# TranferedModel.build_vocab([[]]) # list of list\n",
        "# ** list of list 로 넘겨줘야 함.\n",
        "TranferedModel.build_vocab([PreTrainedKeyvector.wv.vocab.keys()])\n",
        "#---------------------------------------------\n",
        "# 4) UPDATE vocab by sentences\n",
        "# update parameter를 True로 설정.\n",
        "TranferedModel.build_vocab(sentences, update=True)\n",
        "#---------------------------------------------\n",
        "# 5) INITIALIZED word vector\n",
        "# vocab에 있는 단어들의 vector를 `googleNews_filepath`에 있는 값으로 모두 업데이트해줌.\n",
        "# lockf=1.0 : 보통은 vector를 update하지 못하도록 lock이 걸려 있는데, 여기서는 이걸 품.\n",
        "TranferedModel.intersect_word2vec_format(\n",
        "    googleNews_filepath, binary=True, lockf=1.0\n",
        ")\n",
        "#---------------------------------------------\n",
        "# 6) TRAIN new data set.\n",
        "print(\"== Before New data training\")\n",
        "print(TranferedModel.wv['boy'][:10])\n",
        "TranferedModel.train(sentences, total_examples=len(sentences), epochs=100)\n",
        "print(\"== After New data training\")\n",
        "print(TranferedModel.wv['boy'][:10])"
      ],
      "metadata": {
        "id": "LU5Q5BsZk1eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 word2vec 사용 시\n",
        "import gensim\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "model.most_similar(positive=['good', 'sad'], negative=['bad'])"
      ],
      "metadata": {
        "id": "RKW19ckJk1oK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}