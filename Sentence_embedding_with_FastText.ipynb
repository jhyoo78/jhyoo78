{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPIIZ0fgi+/Pt52E/puXwAr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhyoo78/jhyoo78/blob/main/Sentence_embedding_with_FastText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuKnTvZHUDng"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import zipfile\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import  stopwords                        # 향후 전처리로 to, for 등을 제거할 것"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PC 에서 train.txt(log file 이름)를 upload\n",
        "\n",
        "from google.colab import files     # Colab에서 local PC내의 file 읽어 들이는 라이브러리\n",
        "uploaded = files.upload()          # train.txt 파일을  선택"
      ],
      "metadata": {
        "id": "40yr9g_2mItp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_processed_text = []\n",
        "\n",
        "f = open('train.txt', 'r', encoding='UTF8')          # 1,100 줄의 log 파일\n",
        "lines = f.readlines()\n",
        "\n",
        "for line in lines:\n",
        "    sent_text = sent_tokenize(line)     #  log msg를 줄 단위의 sentence로 토큰화\n",
        "    #print(sent_text)\n",
        "\n",
        "    # 각 문장에 대해서 구두점, LF 등을 제거하고, 대문자를 소문자로 변환.\n",
        "    for string in sent_text:\n",
        "        tokens = re.sub(r\"[^a-z]+\", \" \", string.lower())         # 입력 파일(train.txt)에 있는 숫자를 제거했음(3월 1일에)\n",
        "        #tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "        pre_processed_text.append(tokens)\n",
        "\n",
        "print(\"pre_processed_text=\", pre_processed_text)\n",
        "\n",
        "result=[pre_processed_text]    # 문장 단위로 유사도를 비교할 때 , result=[[['rsyslogd origin softwarersyslogd swversion ', 'systemd starting rotate log files',...]]\n",
        "\n",
        "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
        "#result = [word_tokenize(sentence) for sentence in pre_processed_text]    # 단어의 유사도를 비교하거나... 또는 한 문장과 유사도가 비슷한 단어를 찾을 때, result=[[['rsyslogd', 'origin', 'softwarersyslogd swversion'], ['systemd starting rotate log files'],,...]]\n",
        "print(\"result=\", result)\n",
        "print('총 샘플의 개수 : {}'.format(len(result)))\n"
      ],
      "metadata": {
        "id": "E_Bv4M7wmI9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FastText로 훈련하기---- min_count를 2로 할때 model 크기가 약 44MB, 1로 바꾸어 학습한 경과는 93MB 임. 즉, 한번 등장하는 문장이 많다는 의미...\n",
        "\n",
        "from gensim.models import FastText\n",
        "model = FastText(result, size=200, window=5, min_count=1, workers=4, sg=1)   # default epoch = 5 임. skip_gram이 사용하는 Window size=5\n",
        "\n",
        "model.save('fasttext_train_0301')"
      ],
      "metadata": {
        "id": "goKpjusS1rMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#a= model['logrotateservice succeeded']   # model['abc'] 로 벡터를 구하는 것은 gensim에서 deprecated 되었음.\n",
        "\n",
        "a= model.wv['logrotateservice succeeded']   # get the vector of the word or sentence\n",
        "b= model.wv['logrotateservice']\n",
        "c= model.wv['succeeded']\n",
        "#print(a)\n",
        "print(\"a_sum=\", sum(a))\n",
        "print(\"b_sum=\", sum(b))\n",
        "print(\"c_sum=\", sum(c))\n"
      ],
      "metadata": {
        "id": "cFc38qy91rXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a= model.wv['systemd started message day']   # get the vector of the word or sentence\n",
        "b= model.wv['systemd']\n",
        "c= model.wv['started']\n",
        "d= model.wv['message']\n",
        "e= model.wv['day']\n",
        "\n",
        "print(\"a_sum=\", sum(a))\n",
        "print(\"b_sum=\", sum(b))\n",
        "print(\"c_sum=\", sum(c))\n",
        "print(\"d_sum=\", sum(d))\n",
        "print(\"e_sum=\", sum(e))\n",
        "\n",
        "z= -(0.33092026179656386 +0.09879262174945325 +0.01677472274604952 +0.0649924342869781)/4  # 평균\n",
        "print(z)"
      ],
      "metadata": {
        "id": "kvxcA-6FYEv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('systemd started message  day', topn=5)"
      ],
      "metadata": {
        "id": "6RG5KUSQ1rwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(negative=['systemd started message  day'], topn=5, )"
      ],
      "metadata": {
        "id": "Ioc3b_yBFC6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('fail', topn=5)    # most_similar 명령은 deprecated 됨.  wv.most_similar로 통합됨. --> snetence를 학습했으므로 한 단어를 입력해도 단어가 아닌 유사한 문장이 출력됨."
      ],
      "metadata": {
        "id": "tHjAX7lLrMaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install glove_python  # !pip도 build 오류 발생함, toy로 소개되어 있어서 ???"
      ],
      "metadata": {
        "id": "S447aaXsmJNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R2pipAcGmJTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 구글 Drive에 연결하기"
      ],
      "metadata": {
        "id": "hfL7LEHcP0XK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "TMj1ZIkrmJYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "구글 Drive에 FastText 학습 모델 저장하기"
      ],
      "metadata": {
        "id": "s6-va_LAP4_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/My Drive/Colab Notebooks/fasttext_train_0301') # Colab 좌측 창에 임시 저장된 h5 파일을 다운로드 하여 저장한다."
      ],
      "metadata": {
        "id": "HWvkBo3SmJcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EOF"
      ],
      "metadata": {
        "id": "FnOudFMTQML7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EbX4RJSCmJfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Glove로 훈련하기  --> import\n",
        "from glove import Glove, Corpus\n",
        "#from gensim.models import Glove, Corpus\n",
        "corpus = Corpus()\n",
        "corpus.fit(result, window=5)\n",
        "# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n",
        "\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20."
      ],
      "metadata": {
        "id": "zFuazyDKmJi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bb5ISPcImJmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kpjHmwAlmJqL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}