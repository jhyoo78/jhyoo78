{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhyoo78/jhyoo78/blob/main/log_doc2vec_test_many_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VR5F3kbadR0"
      },
      "outputs": [],
      "source": [
        "from google.colab import files     # Colab에서 local PC내의 file 읽어 들이는 라이브러리\n",
        "uploaded = files.upload()          # 파일을 여러 개 선택 가능함"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V1VeuX-mNgtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import dot    # cosine similarity 계산 용\n",
        "from numpy.linalg import norm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')   # ‘punkt=문장 tokenizer’를 다운로드\n",
        "import urllib.request   # 아나콘다에 urllib3 만 있음\n",
        "#import zipfile           # zipfile36만 있음, 파이선이 3.6 버전이어야 함\n",
        "from lxml import etree\n",
        "import re                # 파이썬 정규표현을 위한 표준 라이브러리\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import pandas as pd      # opencv에 conda install로 설치할 것\n",
        "from gensim.models import Word2Vec     # opencv에  설치할 것\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "3qxQQv_gbe-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_text1 = []     # 또는 list()\n",
        "data_text2 = []\n",
        "\n",
        "'''------------------------첫번째 train 용 파일 처리---------------------------------'''\n",
        "f = open('test1.txt', 'r')\n",
        "lines = f.readlines()\n",
        "for line in lines:\n",
        "\n",
        "    sent_text = sent_tokenize(line)   # 각 sentence 를 (예: 'starting rotate log files') 으로 separation 하기\n",
        "    #print(\"sent_text=\", sent_text)\n",
        "\n",
        "    # 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환. -----> 전처리 기능 추가할 것\n",
        "    normalized_text = []\n",
        "\n",
        "    for string in sent_text:\n",
        "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "        #print(\"token=\", tokens)\n",
        "        data_text1.append(tokens)\n",
        "    #print(data_text1)\n",
        "f.close()\n",
        "print(\"data_tex1t =\", data_text1)\n",
        "\n",
        "'''------------------------- test 용 파일 처리 ----------------------------------'''\n",
        "f = open('test2.txt', 'r')\n",
        "lines = f.readlines()\n",
        "for line in lines:\n",
        "\n",
        "    sent_text = sent_tokenize(line)   # sentence 단위로 (예: 'starting rotate log files') 으로 구분\n",
        "    #print(\"sent_text=\", sent_text)\n",
        "\n",
        "    # 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환. -----> 전처리 기능 추가할 것\n",
        "    normalized_text = []\n",
        "\n",
        "    for string in sent_text:\n",
        "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "        #print(\"token=\", tokens)\n",
        "        data_text2.append(tokens)\n",
        "    #print(data_text1)\n",
        "f.close()\n",
        "print(\"data_text2 = \", data_text2)\n",
        "\n",
        "'''----------------------------  2개의 taggedDocument 생성-----------------------------'''\n",
        "\n",
        "# TaggedDocument 생성 --> sentence 별로 index를 추가함.\n",
        "tagged_data1 = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data_text1)]\n",
        "tagged_data2 = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data_text2)]\n",
        "\n",
        "'''----------------------------------- 확인 용 (삭제해도 됨) ----------------------------------------'''\n",
        "print(tagged_data1)\n",
        "print(tagged_data2)\n",
        "\n",
        "print(tagged_data1[0])\n",
        "#print(tagged_data1[0][1])\n",
        "#print(tagged_data1[0][1].index('0'))"
      ],
      "metadata": {
        "id": "1V1ZM_LwbhLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Doc2Vec 모델 생성하기\n",
        "\n",
        "# Doc2Vec 초기화\n",
        "model1 = Doc2Vec(vector_size=100, alpha=0.025, min_alpha=0.025, min_count=1, epochs=100, dm =1)   # dm=1이면 PV-DM, 0 이면 PV-DBOW, default epochs=10임\n",
        "model2 = Doc2Vec(vector_size=100, alpha=0.025, min_alpha=0.025, min_count=1, epochs=100, dm =1)\n",
        "#  tagged_data의 vocab 구축하기\n",
        "model1.build_vocab(tagged_data1)    # model의 vocabulary를 tagged_data 라는 이름으로 구축한다.\n",
        "model2.build_vocab(tagged_data2)\n",
        "#print(tagged_data1)\n",
        "\n",
        "# Doc2Vec(tagged_data) 훈련하기\n",
        "model1.train(tagged_data1, total_examples = model1.corpus_count, epochs = model1.epochs)\n",
        "model2.train(tagged_data2, total_examples = model2.corpus_count, epochs = model2.epochs)\n",
        "\n"
      ],
      "metadata": {
        "id": "vh7c67XFsEeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 모델 저장하기\n",
        "model1.save('log.doc2vec')"
      ],
      "metadata": {
        "id": "e6c3d2CHH_o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''------------------------------------ log 분석하기 --------------------------------------------------'''\n",
        "# Tag(index)를 이용한 유사 sentence 찾기 --- 전처리 과정에서 생략되는 단어/문장도 있으므로 원본 data가 아닌 tagged_data에서 찾아야 함.\n",
        "\n",
        "'''------------Cosine similarity 계산 함수 ----------------'''\n",
        "def cos_sim(A, B):\n",
        "  return dot(A, B)/(norm(A)*norm(B))\n",
        "''' -------------------------------------------------------'''\n",
        "\n",
        "tags = ['146', '209', '910', '1028']   # test1.txt: 146번 sentence => 'init failed run of stage init'   'failed'를 포함한 문장들\n",
        "topn = 3\n",
        "faults_sim=[]\n",
        "\n",
        "'''======================------------------ 고장 메시지와의 유사도 계산 ---------------------=============================='''\n",
        "for idx, tag1 in enumerate(tags):\n",
        "    ''' -------------------------- 이하는 확인 용도 (생략 가능) --------------------------------------------------------------------------------'''\n",
        "    print(\"target index= \", tag1, \" \", \"sentence =\", tagged_data1[int(tag1)][0])\n",
        "    similar_sentences_topn = model1.docvecs.most_similar(tag1, topn = topn)  # 훈련된 model1의 tag(index) 로 상위 topn(예: 3개)를 예측한다, 실행할 때마다 결과가 일부 변경됨\n",
        "    print(\"similar sentences with index and similarity =\", similar_sentences_topn)\n",
        "    print(\"\")\n",
        "\n",
        "    for n in range(topn):\n",
        "        tuple_idx = similar_sentences_topn[n][0]\n",
        "        tuple_sim = similar_sentences_topn[n][1]\n",
        "        #print(\"similar sentence=\", \"index:\", tuple_idx, \" \", tuple_sim, \" \", tagged_data1[int(tuple_idx)][0])    # 아직 미사용\n",
        "\n",
        "    print(\"\")\n",
        "\n",
        "    '''----- 여기에서 for loop로 text2 와 비교하여 유사도를 계산함, test2.txt를 학습하지 않고(model 생성 없이) 비교하면 '없는 문장' 오류 발생 -----'''\n",
        "\n",
        "    pv1= model1.docvecs[tag1]     # model1의 paragraph vector 추출하기\n",
        "\n",
        "    rng = len(tagged_data2)  # range\n",
        "    #print(rng)      # rng= 문장의 수, 그래프에서 x 축으로 표시됨\n",
        "    flt_sim = []\n",
        "\n",
        "    for s in range(rng):   #  계산\n",
        "        pv2 = model2.docvecs[str(s)]  # 시험할 model2의 paragraph vector를 순서대로 추출\n",
        "        cos_similarity = cos_sim(np.array(pv1), np.array(pv2))  # 두 개 벡터(pv1, pv2)의 Cosine 유사도 계산\n",
        "        flt_sim.append(cos_similarity)\n",
        "    faults_sim.append(flt_sim)\n",
        "\n",
        "print(faults_sim[0])\n",
        "print(len(faults_sim[1]))\n"
      ],
      "metadata": {
        "id": "P0G6eXN2h81P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tap1iwgxh9MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시험 데이터(문장) 각각에 대해 위 4개의 문장 ['146', '209', '910', '1028']에 대한 유사도를 plot함\n",
        "x_axis = len(tagged_data2)\n",
        "\n",
        "# 아래 명령의 순서를 바꾸면 빈 그래프도 하나 출력되는 이상한 현상이 나타남.\n",
        "plt.figure(figsize = (20,10))\n",
        "plt.axis([ 0, x_axis, 0, 1.2 ])    # x-y 축 지정\n",
        "plt.plot(np.arange(x_axis), faults_sim[0], 'ro')    # ([x축의 값],[ y 축의 값]) ,  ro => red 문자   ==> for loop로 수정, input window 도 적용, 누적 그래프도....\n",
        "plt.plot(np.arange(x_axis), faults_sim[1], 'bo')\n",
        "plt.plot(np.arange(x_axis), faults_sim[2], 'go')\n",
        "plt.plot(np.arange(x_axis), faults_sim[3], 'yo')\n",
        "plt.xlabel('sequence number')\n",
        "plt.ylabel('similarity')\n",
        "plt.title('Failure Prediction with failure logs')\n",
        "#plt.savefig('test.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yTvxpRn0_M54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EOF"
      ],
      "metadata": {
        "id": "4aeyJ8L8RxaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' ---------------------- Cosine 유사도 계산 --------------------------'''\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cos_sim(A, B):\n",
        "  return dot(A, B)/(norm(A)*norm(B))\n",
        "\n",
        "doc1 = np.array([0,1,1,1])\n",
        "doc2 = np.array([1,0,1,1])\n",
        "doc3 = np.array([2,0,2,2])\n",
        "\n",
        "print('doc1과 doc2의 similarity =',cos_sim(doc1, doc2))\n",
        "print('doc1과 doc3의 similarity =',cos_sim(doc1, doc3))\n",
        "print('doc2와 doc3의 similarity =',cos_sim(doc2, doc3))   # 벡터의 방향이 같으면 1이 출력됨.\n",
        "print(\"\")\n",
        "\n",
        "print('vectors의 유사도 :', cos_sim(np.array(v1), np.array(pv)))"
      ],
      "metadata": {
        "id": "3H6xn8pobJQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_doc = model1.docvecs.most_similar('146', topn=2)\n",
        "print(similar_doc)"
      ],
      "metadata": {
        "id": "PWGQdShzJLT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pv = model1.docvecs['146']   # tag에 대한 paragraph vector 를 출력, 훈련된 모든 vector의 객체임\n",
        "print(\"paragraph vector =\", pv)\n",
        "\n",
        "test_data1 = word_tokenize('init failed run of stage init')    # 데이터에 없는 단어의 embedding, 실행할 때마다 값이 변화함.\n",
        "v1 = model1.infer_vector(test_data1, alpha=0.025, min_alpha=0.025, epochs=100)\n",
        "print(\"Inferred vector=\", v1)\n",
        "\n",
        "test_data2 = word_tokenize('init started on')  # 시험용, tag='146'과 다른 메시지인 경우\n",
        "\n",
        "#cross_sim = model.docvecs.similarity(tag, str(s))\n",
        "cross_sim = model1.wv.n_similarity(test_data1, test_data2)   # tokenized words를 입력하여 유사도를 계산함\n",
        "print(cross_sim)\n"
      ],
      "metadata": {
        "id": "u0xBoWGobJea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' ================== log file 2개 읽어 처리하기 ==================='''\n",
        "nums = range(2)\n",
        "for num in nums:\n",
        "    path = 'C:/Users/YOO/Desktop/장애예측/Log_exampleafter_pre_processing/'test%d.txt' %num   #=====> Colab에서는 사용 불가\n",
        "    data = pd.read_csv(path, sep = \"\\t\", , engine='python', encoding = \"cp949\")\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "FfBiFsO4bJo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oNPuA3ymvcST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C5zXZ9l6sPYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래에서는 PCA를 참고....."
      ],
      "metadata": {
        "id": "u7KgCCQERowa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=token_text, size=10, window=5, min_count=2, workers=4, iter=2, sg=0)   # worker== 사용할 CPU core의 개수\n",
        "print(\"model=\", model.wv)\n",
        "print(\"wor2vector=\", model.wv['errorh'])     # 특정 단어를 size 차원의 벡터로 변환하기\n",
        "\n",
        "model_result = model.wv.most_similar(\"errorh\", topn=10)   # 다른 유사한 단어(상위 10 개)와 유사도 계산\n",
        "#model_result2 = model.wv.similarity(w1=\"failed\", w2= \"out\")    # 2개 단어 간의 유사도 계산\n",
        "print(\"model_result=\", model_result)\n",
        "#print(\"model_result2=\", model_result2)\n",
        "print('')\n",
        "\n",
        "\n",
        "'''------------ 학습 결과 그리기 ==> https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=wideeyed&logNo=221349385092 ---------'''\n",
        "def plot_2d_graph(vocabs, xs, ys):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(xs, ys, marker = 'o')\n",
        "    for i, v in enumerate(vocabs):\n",
        "        plt.annotate(v, xy=(xs[i], ys[i]))\n",
        "\n",
        "vocabs = model.wv.vocab.keys()\n",
        "word_vectors_list=[model.wv[v] for v in vocabs]\n",
        "\n",
        "'''--- 고차원 벡터를 시각화하기 위해서는 2~3차원 벡터로 축소하는 것이 필요하다. PCA는 대표적인 차원 축소 방식 중 하나임------'''\n",
        "''' -------------------- PCA는 군집을 표현 못하니 t-SNE로 바꾸어 볼 것 ---------------------'''\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca= PCA(n_components=2)\n",
        "xys = pca.fit_transform(word_vectors_list)\n",
        "print(\"xys=\", xys)\n",
        "print(len(xys))       # xys의 [] 길이는 단어의 개수와 같음, 예 : xys= [[-6.54008631e-02 -1.85528987e-02]....... [-2.234521e-02  3.39717996e-02]]\n",
        "xs = xys[:, 0]\n",
        "ys = xys[:, 1]\n",
        "plot_2d_graph(vocabs, xs, ys)\n"
      ],
      "metadata": {
        "id": "CklhIrVQ47Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.wv.vocab)   # 어휘사전은 단어의 빈도 순으로 출력 ???"
      ],
      "metadata": {
        "id": "wj6vL4QoFUXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''---------- 아래 코드는 https://stackoverflow.com/questions/31440803/how-to-fetch-vectors-for-a-word-list-with-word2vec 에서 인용--------- '''\n",
        "''' ---------- 전체 vectors for word list 를 출력 ---------------'''\n",
        "my_dict = dict({})\n",
        "for idx, key in enumerate(model.wv.vocab):\n",
        "    my_dict[key] = model.wv[key]\n",
        "    # Or my_dict[key] = model.wv.get_vector(key)\n",
        "    # Or my_dict[key] = model.wv.word_vec(key, use_norm=False)\n",
        "\n",
        "#%store my_dict > filename.txt   # 파일로 저장하기"
      ],
      "metadata": {
        "id": "bDvEVr4q4-No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_dict)\n",
        "# 단어의 수\n",
        "len(model.wv.vocab)"
      ],
      "metadata": {
        "id": "52mJ5JnN4-sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QS9PtQajLJR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "data = [\"I love machine learning. Its awesome.\",\n",
        "        \"I love coding in python\",\n",
        "        \"I love building chatbots\",\n",
        "        \"they chat amagingly well\"]\n",
        "\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]    # enumerate(x)는 x의 index와 data를 분리한다.\n",
        "\n",
        "print(tagged_data)"
      ],
      "metadata": {
        "id": "Q_A2wxxIwsso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.arange(12)  # ==> [0,1,2...11]\n",
        "x = x.reshape(3,4)\n",
        "\n",
        "print(x.reshape(-1,1))   # 1차원 배열의 원소를 1개로\n",
        "\n",
        "print(x.reshape(-1,2))\n",
        "print(x.reshape(-1,3)) # 1차원 배열의 원소를 3개로\n",
        "\n",
        "x.reshape(-1)          # 1차원으로 출력"
      ],
      "metadata": {
        "id": "tlxZNmSJbhcF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}