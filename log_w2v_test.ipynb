{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOIiNyZWv7KwgyVaeYdGwun",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhyoo78/jhyoo78/blob/main/log_w2v_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VR5F3kbadR0"
      },
      "outputs": [],
      "source": [
        "from google.colab import files     # Colab에서 local file 읽어 들이는 라이브러리\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')   # ‘punkt=문장 tokenizer’를 다운로드\n",
        "import urllib.request   # 아나콘다에 urllib3 만 있음\n",
        "#import zipfile           # zipfile36만 있음, 파이선이 3.6 버전이어야 함\n",
        "from lxml import etree   \n",
        "import re                # 파이썬 정규표현을 위한 표준 라이브러리임\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import pandas as pd      # opencv에 conda install로 설치할 것\n",
        "from gensim.models import Word2Vec     # opencv에  설치할 것\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3qxQQv_gbe-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_text = []     # 또는 list()\n",
        "\n",
        "f = open('test.txt', 'r')\n",
        "lines = f.readlines()\n",
        "for line in lines:\n",
        "\n",
        "    sent_text = sent_tokenize(line)\n",
        "    #print(sent_text)\n",
        "   \n",
        "    # 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
        "    normalized_text = []\n",
        "    \n",
        "    for string in sent_text:\n",
        "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "        normalized_text.append(tokens)\n",
        "        #print(normalized_text)\n",
        "        \n",
        "    # 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
        "    result = [word_tokenize(sentence) for sentence in normalized_text]\n",
        "    result_arr= np.array(result).reshape(-1)  # List에서는 reshpe()를 사용 못하므로 array로 변환하여 처리함. reshape(-1)은 2차원 배열에서 1차원 배열만 출력함. 즉 [[ ]] --> [ ]\n",
        "    #print(type(result_arr))  # ==> <class 'numpy.ndarray'>로 출력됨\n",
        "    result_list=result_arr.tolist()   # array를 list 로 다시 변환\n",
        "    #print(\"result=\", result)\n",
        "    #print(\"result_arr=\", result_arr)     # [  ]\n",
        "    token_text.append(result_list)        # token_text=[] 에 append()\n",
        "    \n",
        "#print('총 샘플의 개수 : {}'.format(len(token_text)))\n",
        "print(type(token_text))\n",
        "print(\"tokenized_text=\", token_text)\n",
        "\n",
        "for line in token_text[:3]:    # 앞의 3줄만 출력해 봄.\n",
        "    print(line)\n",
        "\n",
        "#print(\"tokenized text=\", token_text)\n",
        "#print(token_text[0][:])\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "1V1ZM_LwbhLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=token_text, size=10, window=5, min_count=2, workers=4, iter=2, sg=0)   # worker== 사용할 CPU core의 개수\n",
        "print(\"model=\", model.wv)\n",
        "print(\"wor2vector=\", model.wv['errorh'])     # 특정 단어를 size 차원의 벡터로 변환하기\n",
        "\n",
        "model_result = model.wv.most_similar(\"errorh\", topn=10)   # 다른 유사한 단어(상위 10 개)와 유사도 계산\n",
        "#model_result2 = model.wv.similarity(w1=\"failed\", w2= \"out\")    # 2개 단어 간의 유사도 계산\n",
        "print(\"model_result=\", model_result)\n",
        "#print(\"model_result2=\", model_result2)\n",
        "print('')\n",
        "\n",
        "\n",
        "'''------------ 학습 결과 그리기 ==> https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=wideeyed&logNo=221349385092 ---------'''\n",
        "def plot_2d_graph(vocabs, xs, ys):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(xs, ys, marker = 'o')\n",
        "    for i, v in enumerate(vocabs):\n",
        "        plt.annotate(v, xy=(xs[i], ys[i]))\n",
        "\n",
        "vocabs = model.wv.vocab.keys()\n",
        "word_vectors_list=[model.wv[v] for v in vocabs]\n",
        "\n",
        "'''--- 고차원 벡터를 시각화하기 위해서는 2~3차원 벡터로 축소하는 것이 필요하다. PCA는 대표적인 차원 축소 방식 중 하나임------'''\n",
        "''' -------------------- PCA는 군집을 표현 못하니 t-SNE로 바꾸어 볼 것 ---------------------'''\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca= PCA(n_components=2)\n",
        "xys = pca.fit_transform(word_vectors_list)\n",
        "print(\"xys=\", xys)\n",
        "print(len(xys))       # xys의 [] 길이는 단어의 개수와 같음, 예 : xys= [[-6.54008631e-02 -1.85528987e-02]....... [-2.234521e-02  3.39717996e-02]]\n",
        "xs = xys[:, 0]\n",
        "ys = xys[:, 1]\n",
        "plot_2d_graph(vocabs, xs, ys)\n"
      ],
      "metadata": {
        "id": "CklhIrVQ47Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.wv.vocab)   # 어휘사전은 단어의 빈도 순으로 출력 ???"
      ],
      "metadata": {
        "id": "wj6vL4QoFUXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''---------- 아래 코드는 https://stackoverflow.com/questions/31440803/how-to-fetch-vectors-for-a-word-list-with-word2vec 에서 인용--------- '''\n",
        "''' ---------- 전체 vectors for word list 를 출력 ---------------'''\n",
        "my_dict = dict({})\n",
        "for idx, key in enumerate(model.wv.vocab):\n",
        "    my_dict[key] = model.wv[key]\n",
        "    # Or my_dict[key] = model.wv.get_vector(key)\n",
        "    # Or my_dict[key] = model.wv.word_vec(key, use_norm=False)\n",
        "\n",
        "#%store my_dict > filename.txt   # 파일로 저장하기"
      ],
      "metadata": {
        "id": "bDvEVr4q4-No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_dict)"
      ],
      "metadata": {
        "id": "52mJ5JnN4-sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어의 수\n",
        "len(model.wv.vocab)"
      ],
      "metadata": {
        "id": "SBBjGUOJ4-1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.wv.vocab)\n",
        "model.wv.get_vector('log')"
      ],
      "metadata": {
        "id": "rYZHwJpQG1YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mc6OXjaJG1jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습한 모델 저장 및 불러오기....\n",
        "model.save(file name)\n",
        ">>> model = Word2Vec.load(file name)  # you can continue training with the loaded model!"
      ],
      "metadata": {
        "id": "jqgf5Xp6G1sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('test.txt', 'r')\n",
        "file_read = f.read()   # 줄단위는 read_limes(), 전체는 read()\n",
        "#print(file_read)\n",
        "\n",
        "sent_text = sent_tokenize(file_read)\n",
        "print(sent_text)   # 전체를 하나의 벡터로 표시하고 \\n으로 문장을 구분함 ['a b c...\\n a ab c ...']\n",
        "\n",
        "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
        "normalized_text = []\n",
        "    \n",
        "for string in sent_text:\n",
        "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "    normalized_text.append(tokens)\n",
        "    print(normalized_text)\n",
        "\n",
        "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]\n",
        "print(result)\n",
        "#print('총 샘플의 개수 : {}'.format(len(result)))  # ==> 코드 오류\n",
        "\n",
        "#for line in result[0:3]:\n",
        "#    print(line)\n",
        "\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "Cut4eUt0bhTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.arange(12)  # ==> [0,1,2...11]\n",
        "x = x.reshape(3,4)\n",
        "\n",
        "print(x.reshape(-1,1))   # 1차원 배열의 원소를 1개로\n",
        "\n",
        "print(x.reshape(-1,2))\n",
        "print(x.reshape(-1,3)) # 1차원 배열의 원소를 3개로\n",
        "\n",
        "x.reshape(-1)          # 1차원으로 출력"
      ],
      "metadata": {
        "id": "tlxZNmSJbhcF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}